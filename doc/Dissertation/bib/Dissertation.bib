@article{Fujisawa2019,
abstract = {Background: Application of deep-learning technology to skin cancer classification can potentially improve the sensitivity and specificity of skin cancer screening, but the number of training images required for such a system is thought to be extremely large. Objectives: To determine whether deep-learning technology could be used to develop an efficient skin cancer classification system with a relatively small dataset of clinical images. Methods: A deep convolutional neural network (DCNN) was trained using a dataset of 4867 clinical images obtained from 1842 patients diagnosed with skin tumours at the University of Tsukuba Hospital from 2003 to 2016. The images consisted of 14 diagnoses, including both malignant and benign conditions. Its performance was tested against 13 board-certified dermatologists and nine dermatology trainees. Results: The overall classification accuracy of the trained DCNN was 76{\textperiodcentered}5{\%}. The DCNN achieved 96{\textperiodcentered}3{\%} sensitivity (correctly classified malignant as malignant) and 89{\textperiodcentered}5{\%} specificity (correctly classified benign as benign). Although the accuracy of malignant or benign classification by the board-certified dermatologists was statistically higher than that of the dermatology trainees (85{\textperiodcentered}3{\%} ± 3{\textperiodcentered}7{\%} and 74{\textperiodcentered}4{\%} ± 6{\textperiodcentered}8{\%}, P {\textless} 0{\textperiodcentered}01), the DCNN achieved even greater accuracy, as high as 92{\textperiodcentered}4{\%} ± 2{\textperiodcentered}1{\%} (P {\textless} 0{\textperiodcentered}001). Conclusions: We have developed an efficient skin tumour classifier using a DCNN trained on a relatively small dataset. The DCNN classified images of skin tumours more accurately than board-certified dermatologists. Collectively, the current system may have capabilities for screening purposes in general medical practice, particularly because it requires only a single clinical image for classification.},
author = {Fujisawa, Y. and Otomo, Y. and Ogata, Y. and Nakamura, Y. and Fujita, R. and Ishitsuka, Y. and Watanabe, R. and Okiyama, N. and Ohara, K. and Fujimoto, M.},
doi = {10.1111/bjd.16924},
issn = {13652133},
journal = {British Journal of Dermatology},
month = feb,
number = {2},
pages = {373--381},
publisher = {Blackwell Publishing Ltd},
title = {Deep-learning-based, computer-aided classifier developed with a small dataset of clinical images surpasses board-certified dermatologists in skin tumour diagnosis},
volume = {180},
year = {2019}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
month = mar,
title = {Distilling the Knowledge in a Neural Network},
year = {2015}
}
@inproceedings{Baylor2017,
abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components-a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components , simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2{\%} increase in app installs resulting from improved data and model analysis.},
author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and {Yu Foo}, Chuan and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and {Yuen Koo}, Chiu and Lew, Lukasz and Mewald, Clemens and {Naresh Modi}, Akshay and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and {Euijong Whang}, Steven and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
doi = {10.1145/3097983.3098021},
title = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
year = {2017}
}
@article{Barata2020,
author = {Barata, Catarina and Celebi, M. Emre and Marques, Jorge S.},
doi = {10.1016/j.patcog.2020.107413},
issn = {00313203},
journal = {Pattern Recognition},
month = may,
pages = {107413},
publisher = {Pergamon},
title = {Explainable skin lesion diagnosis using taxonomies},
year = {2020}
}
@article{talbot,
abstract = {Model selection strategies for machine learning algorithms typically involve the numerical opti-misation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.},
author = {Cawley, Gavin C and Talbot, Nicola L C},
journal = {Journal of Machine Learning Research},
keywords = {bias-variance trade-off,model selection,over-fitting,performance evaluation,selection bias},
pages = {2079--2107},
title = {On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},
volume = {11},
year = {2010}
}
@article{ham10000,
abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50{\%} of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
archivePrefix = {arXiv},
arxivId = {1803.10417},
author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
doi = {10.1038/sdata.2018.161},
eprint = {1803.10417},
issn = {20524463},
journal = {Scientific Data},
month = aug,
publisher = {Nature Publishing Groups},
title = {The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
volume = {5},
year = {2018}
}
@article{Mishra2016,
abstract = {The incidence of malignant melanoma continues to increase worldwide. This cancer can strike at any age; it is one of the leading causes of loss of life in young persons. Since this cancer is visible on the skin, it is potentially detectable at a very early stage when it is curable. New developments have converged to make fully automatic early melanoma detection a real possibility. First, the advent of dermoscopy has enabled a dramatic boost in clinical diagnostic ability to the point that melanoma can be detected in the clinic at the very earliest stages. The global adoption of this technology has allowed accumulation of large collections of dermoscopy images of melanomas and benign lesions validated by histopathology. The development of advanced technologies in the areas of image processing and machine learning have given us the ability to allow distinction of malignant melanoma from the many benign mimics that require no biopsy. These new technologies should allow not only earlier detection of melanoma, but also reduction of the large number of needless and costly biopsy procedures. Although some of the new systems reported for these technologies have shown promise in preliminary trials, widespread implementation must await further technical progress in accuracy and reproducibility. In this paper, we provide an overview of computerized detection of melanoma in dermoscopy images. First, we discuss the various aspects of lesion segmentation. Then, we provide a brief overview of clinical feature segmentation. Finally, we discuss the classification stage where machine learning algorithms are applied to the attributes generated from the segmented features to predict the existence of melanoma.},
archivePrefix = {arXiv},
arxivId = {1601.07843},
author = {Mishra, Nabin K. and Celebi, M. Emre},
eprint = {1601.07843},
month = jan,
title = {An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning},
year = {2016}
}
@article{humanvsisic2018,
abstract = {Background: Whether machine-learning algorithms can diagnose all pigmented skin lesions as accurately as human experts is unclear. The aim of this study was to compare the diagnostic accuracy of state-of-the-art machine-learning algorithms with human readers for all clinically relevant types of benign and malignant pigmented skin lesions. Methods: For this open, web-based, international, diagnostic study, human readers were asked to diagnose dermatoscopic images selected randomly in 30-image batches from a test set of 1511 images. The diagnoses from human readers were compared with those of 139 algorithms created by 77 machine-learning labs, who participated in the International Skin Imaging Collaboration 2018 challenge and received a training set of 10 015 images in advance. The ground truth of each lesion fell into one of seven predefined disease categories: intraepithelial carcinoma including actinic keratoses and Bowen's disease; basal cell carcinoma; benign keratinocytic lesions including solar lentigo, seborrheic keratosis and lichen planus-like keratosis; dermatofibroma; melanoma; melanocytic nevus; and vascular lesions. The two main outcomes were the differences in the number of correct specific diagnoses per batch between all human readers and the top three algorithms, and between human experts and the top three algorithms. Findings: Between Aug 4, 2018, and Sept 30, 2018, 511 human readers from 63 countries had at least one attempt in the reader study. 283 (55{\textperiodcentered}4{\%}) of 511 human readers were board-certified dermatologists, 118 (23{\textperiodcentered}1{\%}) were dermatology residents, and 83 (16{\textperiodcentered}2{\%}) were general practitioners. When comparing all human readers with all machine-learning algorithms, the algorithms achieved a mean of 2{\textperiodcentered}01 (95{\%} CI 1{\textperiodcentered}97 to 2{\textperiodcentered}04; p{\textless}0{\textperiodcentered}0001) more correct diagnoses (17{\textperiodcentered}91 [SD 3{\textperiodcentered}42] vs 19{\textperiodcentered}92 [4{\textperiodcentered}27]). 27 human experts with more than 10 years of experience achieved a mean of 18{\textperiodcentered}78 (SD 3{\textperiodcentered}15) correct answers, compared with 25{\textperiodcentered}43 (1{\textperiodcentered}95) correct answers for the top three machine algorithms (mean difference 6{\textperiodcentered}65, 95{\%} CI 6{\textperiodcentered}06–7{\textperiodcentered}25; p{\textless}0{\textperiodcentered}0001). The difference between human experts and the top three algorithms was significantly lower for images in the test set that were collected from sources not included in the training set (human underperformance of 11{\textperiodcentered}4{\%}, 95{\%} CI 9{\textperiodcentered}9–12{\textperiodcentered}9 vs 3{\textperiodcentered}6{\%}, 0{\textperiodcentered}8–6{\textperiodcentered}3; p{\textless}0{\textperiodcentered}0001). Interpretation: State-of-the-art machine-learning classifiers outperformed human experts in the diagnosis of pigmented skin lesions and should have a more important role in clinical practice. However, a possible limitation of these algorithms is their decreased performance for out-of-distribution images, which should be addressed in future research. Funding: None.},
author = {Tschandl, Philipp and Codella, Noel and Akay, Beng{\"{u}} Nisa and Argenziano, Giuseppe and Braun, Ralph P. and Cabo, Horacio and Gutman, David and Halpern, Allan and Helba, Brian and Hofmann-Wellenhof, Rainer and Lallas, Aimilios and Lapins, Jan and Longo, Caterina and Malvehy, Josep and Marchetti, Michael A. and Marghoob, Ashfaq and Menzies, Scott and Oakley, Amanda and Paoli, John and Puig, Susana and Rinner, Christoph and Rosendahl, Cliff and Scope, Alon and Sinz, Christoph and Soyer, H. Peter and Thomas, Luc and Zalaudek, Iris and Kittler, Harald},
doi = {10.1016/S1470-2045(19)30333-X},
issn = {14745488},
journal = {The Lancet Oncology},
month = jul,
number = {7},
pages = {938--947},
publisher = {Lancet Publishing Group},
title = {Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classification: an open, web-based, international, diagnostic study},
volume = {20},
year = {2019}
}
@article{Hennemann2017,
abstract = {eHealth interventions can be effective in treating health problems. However, adoption in inpatient routine care seems limited. The present study therefore aimed to investigate barriers and facilitators to acceptance of eHealth interventions and of online aftercare in particular in health professionals of inpatient treatment. A total of 152 out of 287 health professionals of various professional groups in four inpatient rehabilitation facilities filled out a self-administered web-based questionnaire (response rate: 53{\%}); 128 individuals were eligible for further data analysis. Acceptance and possible predictors were investigated with a complex research model based on the Unified Theory of Acceptance and Use of Technology. Acceptance of eHealth interventions was rather low (M = 2.47, SD = 0.98); however, acceptance of online aftercare was moderate (M = 3.08, SD = 0.96, t(127) = 8.22, p {\textless}.001), and eHealth literacy was elevated. Social influence, performance expectancy, and treatment-related internet and mobile use significantly predicted overall acceptance. No differences were found between professional and age groups. Although acceptance of eHealth interventions was limited in health professionals of inpatient treatment, moderate acceptance of online aftercare for work-related stress implies a basis for future implementation. Tailored eHealth education addressing misconceptions about inferiority and incongruity with conventional treatment considering the systemic aspect of acceptance formation are needed.},
author = {Hennemann, Severin and Beutel, Manfred E. and Zwerenz, R{\"{u}}diger},
doi = {10.1080/10810730.2017.1284286},
issn = {10870415},
journal = {Journal of Health Communication},
month = mar,
number = {3},
pages = {274--284},
pmid = {28248626},
publisher = {Taylor and Francis Inc.},
title = {Ready for eHealth? Health Professionals' Acceptance and Adoption of eHealth Interventions in Inpatient Routine Care},
volume = {22},
year = {2017}
}
@article{Rogers2015,
abstract = {IMPORTANCE: Understanding skin cancer incidence is critical for planning prevention and treatment strategies and allocating medical resources. However, owing to lack of national reporting and previously nonspecific diagnosis classification, accurate measurement of the US incidence of nonmelanoma skin cancer (NMSC) has been difficult. OBJECTIVE: To estimate the incidence of NMSC (keratinocyte carcinomas) in the US population in 2012 and the incidence of basal cell carcinoma (BCC) and squamous cell carcinoma (SCC) in the 2012 Medicare fee-for-service population. DESIGN, SETTING, AND PARTICIPANTS: This study analyzes US government administrative data including the Centers for Medicare {\&} Medicaid Services Physicians Claims databases to calculate totals of skin cancer procedures performed for Medicare beneficiaries from 2006 through 2012 and related parameters. The population-based National Ambulatory Medical Care Survey database was used to estimate NMSC-related office visits for 2012.We combined these analyses to estimate totals of new skin cancer diagnoses and affected individuals in the overall US population. MAIN OUTCOMES AND MEASURES: Incidence of NMSC in the US population in 2012 and BCC and SCC in the 2012 Medicare fee-for-service population. RESULTS: The total number of procedures for skin cancer in the Medicare fee-for-service population increased by 13{\%}from 2 048 517 in 2006 to 2 321 058 in 2012. The age-adjusted skin cancer procedure rate per 100 000 beneficiaries increased from 6075 in 2006 to 7320 in 2012. The number of procedures in Medicare beneficiaries specific for NMSC increased by 14{\%}from 1 918 340 in 2006 to 2 191 100 in 2012. The number of persons with at least 1 procedure for NMSC increased by 14{\%}(from 1 177 618 to 1 336 800) from 2006 through 2012. In the 2012 Medicare fee-for-service population, the age-adjusted procedure rate for BCC and SCC were 3280 and 3278 per 100 000 beneficiaries, respectively. The ratio of BCC to SCC treated in Medicare beneficiaries was 1.0.We estimate the total number of NMSCs in the US population in 2012 at 5 434 193 and the total number of persons in the United States treated for NMSC at 3 315 554. CONCLUSIONS AND RELEVANCE: This study is a thorough nationwide estimate of the incidence of NMSC and provides evidence of continued increases in numbers of skin cancer diagnoses and affected patients in the United States. This study also demonstrates equal incidence rates for BCC and SCC in the Medicare population.},
author = {Rogers, Howard W. and Weinstock, Martin A. and Feldman, Steven R. and Coldiron, Brett M.},
doi = {10.1001/jamadermatol.2015.1187},
issn = {21686068},
journal = {JAMA Dermatology},
month = oct,
number = {10},
pages = {1081--1086},
publisher = {American Medical Association},
title = {Incidence estimate of nonmelanoma skin cancer (keratinocyte carcinomas) in the us population, 2012},
volume = {151},
year = {2015}
}
@article{Tschandl2019,
abstract = {Importance: Convolutional neural networks (CNNs) achieve expert-level accuracy in the diagnosis of pigmented melanocytic lesions. However, the most common types of skin cancer are nonpigmented and nonmelanocytic, and are more difficult to diagnose. Objective: To compare the accuracy of a CNN-based classifier with that of physicians with different levels of experience. Design, Setting, and Participants: A CNN-based classification model was trained on 7895 dermoscopic and 5829 close-up images of lesions excised at a primary skin cancer clinic between January 1, 2008, and July 13, 2017, for a combined evaluation of both imaging methods. The combined CNN (cCNN) was tested on a set of 2072 unknown cases and compared with results from 95 human raters who were medical personnel, including 62 board-certified dermatologists, with different experience in dermoscopy. Main Outcomes and Measures: The proportions of correct specific diagnoses and the accuracy to differentiate between benign and malignant lesions measured as an area under the receiver operating characteristic curve served as main outcome measures. Results: Among 95 human raters (51.6{\%} female; mean age, 43.4 years; 95{\%} CI, 41.0-45.7 years), the participants were divided into 3 groups (according to years of experience with dermoscopy): beginner raters ({\textless}3 years), intermediate raters (3-10 years), or expert raters ({\textgreater}10 years). The area under the receiver operating characteristic curve of the trained cCNN was higher than human ratings (0.742; 95{\%} CI, 0.729-0.755 vs 0.695; 95{\%} CI, 0.676-0.713; P {\textless}.001). The specificity was fixed at the mean level of human raters (51.3{\%}), and therefore the sensitivity of the cCNN (80.5{\%}; 95{\%} CI, 79.0{\%}-82.1{\%}) was higher than that of human raters (77.6{\%}; 95{\%} CI, 74.7{\%}-80.5{\%}). The cCNN achieved a higher percentage of correct specific diagnoses compared with human raters (37.6{\%}; 95{\%} CI, 36.6{\%}-38.4{\%} vs 33.5{\%}; 95{\%} CI, 31.5{\%}-35.6{\%}; P =.001) but not compared with experts (37.3{\%}; 95{\%} CI, 35.7{\%}-38.8{\%} vs 40.0{\%}; 95{\%} CI, 37.0{\%}-43.0{\%}; P =.18). Conclusions and Relevance: Neural networks are able to classify dermoscopic and close-up images of nonpigmented lesions as accurately as human experts in an experimental setting.},
author = {Tschandl, Philipp and Rosendahl, Cliff and Akay, Bengu Nisa and Argenziano, Giuseppe and Blum, Andreas and Braun, Ralph P. and Cabo, Horacio and Gourhant, Jean Yves and Kreusch, J{\"{u}}rgen and Lallas, Aimilios and Lapins, Jan and Marghoob, Ashfaq and Menzies, Scott and Neuber, Nina Maria and Paoli, John and Rabinovitz, Harold S. and Rinner, Christoph and Scope, Alon and Soyer, H. Peter and Sinz, Christoph and Thomas, Luc and Zalaudek, Iris and Kittler, Harald},
doi = {10.1001/jamadermatol.2018.4378},
issn = {21686068},
journal = {JAMA Dermatology},
keywords = {convolutional neural networks,dermoscopy,skin cancer,skin lesion},
month = jan,
number = {1},
pages = {58--65},
publisher = {American Medical Association},
title = {Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks},
volume = {155},
year = {2019}
}
@misc{isic2018top3,
abstract = {Early detection and routine monitoring play a crucial role decreasing the mortality rate of skin cancer. Survival rates decrease significantly if left to be treated in an advanced stage [1]. The ISIC challenges are split into 3 tasks. In this work we only focus on Task 3: Disease Classification. In previous ISIC challenges, classification tasks are comparably easy and less meaningful in a clinical setting. ISIC 2016 [2] required a binary decision of benign vs malignant. ISIC 2017 [3] presented 2 binary classification tasks. The first required a classification of melanoma vs. nevus and seborrheic keratosis, and the second required a classification of seborrheic keratosis vs. melanoma and nevus. The ISIC 2018 challenge [3] [4] demands a much more difficult 7 way classification task which is more representative of a real-world clinical scenario.},
author = {Nozdryn-Plotnicki, Aleksey and Yap, Jordan and Yolland, William},
publisher = {ISIC 2018 Manuscripts},
title = {Ensembling Convolutional Neural Networks for Skin Cancer Classification},
year = {2018}
}
@inproceedings{inceptionv4,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
eprint = {1602.07261},
month = feb,
pages = {4278--4284},
publisher = {AAAI press},
title = {Inception-v4, inception-ResNet and the impact of residual connections on learning},
year = {2017}
}
@misc{Foundation2019,
abstract = {Skin Cancer Facts {\&} Statistics - The Skin Cancer Foundation},
title = {Skin Cancer Facts {\&} Statistics - The Skin Cancer Foundation},
url = {https://skincancer.org/skin-cancer-information/skin-cancer-facts/},
urldate = {2019-11-10},
year = {2019}
}
@inproceedings{snapshot,
abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4{\%} and 17.4{\%} respectively.},
archivePrefix = {arXiv},
arxivId = {1704.00109},
author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1704.00109},
month = apr,
publisher = {ICLR},
title = {Snapshot ensembles: Train 1, get M for free},
year = {2019}
}
@article{Bradshaw2017,
abstract = {Deep neural networks (DNNs) have excellent representative power and are state of the art classifiers on many tasks. However, they often do not capture their own uncertainties well making them less robust in the real world as they overconfidently extrapolate and do not notice domain shift. Gaussian processes (GPs) with RBF kernels on the other hand have better calibrated uncertainties and do not overconfidently extrapolate far from data in their training set. However, GPs have poor representational power and do not perform as well as DNNs on complex domains. In this paper we show that GP hybrid deep networks, GPDNNs, (GPs on top of DNNs and trained end-to-end) inherit the nice properties of both GPs and DNNs and are much more robust to adversarial examples. When extrapolating to adversarial examples and testing in domain shift settings, GPDNNs frequently output high entropy class probabilities corresponding to essentially "don't know". GPDNNs are therefore promising as deep architectures that know when they don't know.},
archivePrefix = {arXiv},
arxivId = {1707.02476},
author = {Bradshaw, John and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
eprint = {1707.02476},
month = jul,
title = {Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks},
year = {2017}
}
@misc{Kittler2002,
abstract = {The accuracy of the clinical diagnosis of cutaneous melanoma with the unaided eye is only about 60{\%}. Dermoscopy, a non-invasive, in vivo technique for the microscopic examination of pigmented skin lesions, has the potential to improve the diagnostic accuracy. Our objectives were to review previous publications, to compare the accuracy of melanoma diagnosis with and without dermoscopy, and to assess the influence of study characteristics on the diagnostic accuracy. We searched for publications between 1987 and 2000 and identified 27 studies eligible for meta-analysis. The diagnostic accuracy for melanoma was significantly higher with dermoscopy than without this technique (log odds ratio 4.0 [95{\%} CI 3.0 to 5.1] versus 2.7 [1.9 to 3.4]; an improvement of 49{\%}, p = 0.001). The diagnostic accuracy of dermoscopy significantly depended on the degree of experience of the examiners. Dermoscopy by untrained or less experienced examiners was no better than clinical inspection without dermoscopy. The diagnostic performance of dermoscopy improved when the diagnosis was made by a group of examiners in consensus and diminished as the prevalence of melanoma increased. A comparison of various diagnostic algorithms for dermoscopy showed no significant differences in their diagnostic performance. A thorough appraisal of the study characteristics showed that most of the studies were potentially influenced by verification bias. In conclusion, dermoscopy improves the diagnostic accuracy for melanoma in comparison with inspection by the unaided eye, but only for experienced examiners.},
author = {Kittler, H. and Pehamberger, H. and Wolff, K. and Binder, M.},
booktitle = {Lancet Oncology},
doi = {10.1016/S1470-2045(02)00679-4},
issn = {14702045},
month = mar,
number = {3},
pages = {159--165},
pmid = {11902502},
publisher = {Lancet Publishing Group},
title = {Diagnostic accuracy of dermoscopy},
volume = {3},
year = {2002}
}
@article{Diaz2017,
abstract = {This report describes our submission to the ISIC 2017 Challenge in Skin Lesion Analysis Towards Melanoma Detection. We have participated in the Part 3: Lesion Classification with a system for automatic diagnosis of nevus, melanoma and seborrheic keratosis. Our approach aims to incorporate the expert knowledge of dermatologists into the well known framework of Convolutional Neural Networks (CNN), which have shown impressive performance in many visual recognition tasks. In particular, we have designed several networks providing lesion area identification, lesion segmentation into structural patterns and final diagnosis of clinical cases. Furthermore, novel blocks for CNNs have been designed to integrate this information with the diagnosis processing pipeline.},
archivePrefix = {arXiv},
arxivId = {1703.01976},
author = {D{\'{i}}az, Iv{\'{a}}n Gonz{\'{a}}lez},
eprint = {1703.01976},
month = mar,
title = {Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagnosis of Skin Lesions},
year = {2017}
}
@article{lin2013network,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
journal = {arXiv preprint},
month = dec,
pages = {10},
title = {Network In Network},
year = {2013}
}
@article{Yuan2017,
abstract = {Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge},
archivePrefix = {arXiv},
arxivId = {1709.09780},
author = {Yuan, Yading and Lo, Yeh-Chi},
eprint = {1709.09780},
month = sep,
title = {Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks},
year = {2017}
}
@inproceedings{Pomponiu2016,
abstract = {Nowadays, the occurrence of skin cancer cases has grown worldwide due to the extended exposure to the harmful radiation from the Sun. Most common approach to detect the malignancy of skin moles is by visual inspection performed by an expert dermatologist, using a set of specific clinical rules. Computer-aided diagnosis, based on skin mole imaging, is another concurrent method which has experienced major advancements due to improvement of imaging sensors and processing power. However, these schemes use hand-crafted features which are difficult to tune and perform poorly on new cases due to lack of generalization power. In this study we present a method that use a pretrained deep neural network (DNN) to automatically extract a set of representative features that can be later used to diagnose a sample of skin lesion for malignancy. The experimental tests carried out on a clinical dataset show that the classification performance using DNN-based features performs better than the state-of-the-art techniques.},
author = {Pomponiu, V. and Nejati, H. and Cheung, N. M.},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7532834},
isbn = {9781467399616},
issn = {15224880},
keywords = {Deep neural networks,Feature extraction,Malignant melanoma,Skin mole classification,Transfer learning},
month = aug,
pages = {2623--2627},
publisher = {IEEE Computer Society},
title = {Deepmole: Deep neural networks for skin mole lesion classification},
volume = {2016-Augus},
year = {2016}
}
@article{Esteva2017,
abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
issn = {0028-0836},
journal = {Nature},
keywords = {Diagnosis,Machine learning,Skin cancer},
month = feb,
number = {7639},
pages = {115--118},
publisher = {Nature Publishing Group},
title = {Dermatologist-level classification of skin cancer with deep neural networks},
volume = {542},
year = {2017}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{https://github.com/fchollet/keras}},
publisher = {GitHub},
title = {Keras},
year = {2015}
}
@article{Perez2018,
abstract = {Deep learning models show remarkable results in automated skin lesion analysis. However, these models demand considerable amounts of data, while the availability of annotated skin lesion images is often limited. Data augmentation can expand the training dataset by transforming input images. In this work, we investigate the impact of 13 data augmentation scenarios for melanoma classification trained on three CNNs (Inception-v4, ResNet, and DenseNet). Scenarios include traditional color and geometric transforms, and more unusual augmentations such as elastic transforms, random erasing and a novel augmentation that mixes different lesions. We also explore the use of data augmentation at test-time and the impact of data augmentation on various dataset sizes. Our results confirm the importance of data augmentation in both training and testing and show that it can lead to more performance gains than obtaining new images. The best scenario results in an AUC of 0.882 for melanoma classification without using external data, outperforming the top-ranked submission (0.874) for the ISIC Challenge 2017, which was trained with additional data.},
archivePrefix = {arXiv},
arxivId = {1809.01442},
author = {Perez, F{\'{a}}bio and Vasconcelos, Cristina and Avila, Sandra and Valle, Eduardo},
doi = {10.1007/978-3-030-01201-4_33},
eprint = {1809.01442},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data augmentation,Deep learning,Skin lesion analysis},
month = sep,
pages = {303--311},
publisher = {Springer Verlag},
title = {Data Augmentation for Skin Lesion Analysis},
volume = {11041 LNCS},
year = {2018}
}
@inproceedings{confpaper,
abstract = {Abstract—Machine learning, specifically, deep learning is a fast-growing field that is being used for multiple medical imaging related problems, such as early detection of skin cancer. For a long time, automated diagnosis of skin cancer from clinical images was considered to be out of reach. However, recent works based on deep networks produced promising results which have the potential to change the landscape of skin lesion diagnosis. Systems created based on these new advancements aim to provide support for both dermatologists in the decision making process and for patients that do not have access to skin professionals. This paper focuses on the current state of automated skin lesion diagnosis using convolutional neural network models, while also providing a comprehensive look into the requirements of integrating such models into a web application capable of helping dermatologists on the diagnosis of skin lesions.},
author = {Santos, F{\'{a}}bio and Silva, Filipe and Georgieva, Petia},
booktitle = {2020 IEEE 10th International Conference on Intelligent Systems (IS)},
keywords = {Automatic assessment tools,eHealth,machine learning,medical imaging},
mendeley-tags = {Automatic assessment tools,eHealth,machine learning,medical imaging},
title = {Automated Diagnosis of Skin Lesions},
year = {2020}
}
@article{gessert2018,
abstract = {In this paper we present the methods of our submission to the ISIC 2018 challenge for skin lesion diagnosis (Task 3). The dataset consists of 10000 images with seven image-level classes to be distinguished by an automated algorithm. We employ an ensemble of convolutional neural networks for this task. In particular, we fine-tune pretrained state-of-the-art deep learning models such as Densenet, SENet and ResNeXt. We identify heavy class imbalance as a key problem for this challenge and consider multiple balancing approaches such as loss weighting and balanced batch sampling. Another important feature of our pipeline is the use of a vast amount of unscaled crops for evaluation. Last, we consider meta learning approaches for the final predictions. Our team placed second at the challenge while being the best approach using only publicly available data.},
archivePrefix = {arXiv},
arxivId = {1808.01694},
author = {Gessert, Nils and Sentker, Thilo and Madesta, Frederic and Schmitz, R{\"{u}}diger and Kniep, Helge and Baltruschat, Ivo and Werner, Ren{\'{e}} and Schlaefer, Alexander},
eprint = {1808.01694},
month = aug,
title = {Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation and Loss Weighting},
year = {2018}
}
@article{Barzegari2005,
abstract = {Background: Computer-aided dermoscopy using artificial neural networks has been reported to be an accurate tool for the evaluation of pigmented skin lesions. We set out to determine the sensitivity and specificity of a computer-aided dermoscopy system for diagnosis of melanoma in Iranian patients. Methods: We studied 122 pigmented skin lesions which were referred for diagnostic evaluation or cosmetic reasons. Each lesion was examined by two clinicians with naked eyes and all of their clinical diagnostic considerations were recorded. The lesions were analyzed using a microDERM{\textregistered} dermoscopy unit. The output value of the software for each lesion was a score between 0 and 10. All of the lesions were excised and examined histologically. Results: Histopathological examination revealed melanoma in six lesions. Considering only the most likely clinical diagnosis, sensitivity and specificity of clinical examination for diagnosis of melanoma were 83{\%} and 96{\%}, respectively. Considering all clinical diagnostic considerations, the sensitivity and specificity were 100{\%} and 89{\%}. Choosing a cut-off point of 7.88 for dermoscopy score, the sensitivity and specificity of the score for diagnosis of melanoma were 83{\%} and 96{\%}, respectively. Setting the cut-off point at 7.34, the sensitivity and specificity were 100{\%} and 90{\%}. Conclusion: The diagnostic accuracy of the dermoscopy system was at the level of clinical examination by dermatologists with naked eyes. This system may represent a useful tool for screening of melanoma, particularly at centers not experienced in the field of pigmented skin lesions. {\textcopyright} 2005 Barzegari et al; licensee BioMed Central Ltd.},
author = {Barzegari, Masoomeh and Ghaninezhad, Haiedeh and Mansoori, Parisa and Taheri, Arash and Naraghi, Zahra S. and Asgari, Masood},
doi = {10.1186/1471-5945-5-8},
issn = {14715945},
journal = {BMC Dermatology},
keywords = {Dermatology,Internal Medicine},
month = jul,
number = {1},
pages = {8},
publisher = {BioMed Central},
title = {Computer-aided dermoscopy for diagnosis of melanoma},
volume = {5},
year = {2005}
}
@article{EmreCelebi2013,
abstract = {Background: Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, automated analysis of dermoscopy images has become an important research area. Border detection is often the first step in this analysis. In many cases, the lesion can be roughly separated from the background skin using a thresholding method applied to the blue channel. However, no single thresholding method appears to be robust enough to successfully handle the wide variety of dermoscopy images encountered in clinical practice. Methods: In this article, we present an automated method for detecting lesion borders in dermoscopy images using ensembles of thres holding methods. Conclusion: Experiments on a difficult set of 90 images demonstrate that the proposed method is robust, fast, and accurate when compared to nine state-of-the-art methods. {\textcopyright} 2012 John Wiley {\&} Sons A/S.},
archivePrefix = {arXiv},
arxivId = {1312.7345},
author = {{Emre Celebi}, M. and Wen, Quan and Hwang, Sae and Iyatomi, Hitoshi and Schaefer, Gerald},
doi = {10.1111/j.1600-0846.2012.00636.x},
eprint = {1312.7345},
issn = {0909752X},
journal = {Skin Research and Technology},
month = feb,
number = {1},
title = {Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding Methods},
volume = {19},
year = {2013}
}
@article{Kawahara2018-7pt,
author = {Kawahara, Jeremy and Daneshvar, Sara and Argenziano, Giuseppe and Hamarneh, Ghassan},
doi = {10.1109/JBHI.2018.2824327},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
month = mar,
number = {2},
pages = {538--546},
publisher = {IEEE},
title = {Seven-point checklist and skin lesion classification using multitask multimodal neural nets},
volume = {23},
year = {2019}
}
@article{He2009,
abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data. {\textcopyright} 2009 IEEE.},
author = {He, Haibo and Garcia, Edwardo A.},
doi = {10.1109/TKDE.2008.239},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Active learning,Assessment metrics,Classification,Cost-sensitive learning,Imbalanced learning,Kernel-based learning,Sampling methods},
month = sep,
number = {9},
pages = {1263--1284},
title = {Learning from imbalanced data},
volume = {21},
year = {2009}
}
@misc{nielsenneural,
author = {Nielsen, Michael A},
keywords = {ba-2018-hahnrico},
publisher = {Determination Press},
title = {Neural Networks and Deep Learning},
type = {misc},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2018}
}
@book{maia,
abstract = {Transfer learning is a popular solution to the common problem in deep learning that is the lack of data or the computational resources to train large models from scratch, which skin lesion classification is a prime candidate for because high quality medical imaging data in this domain is scarce. This dissertation studies transfer learning in the domain of skin lesion classification by exploring pre-trained models of the VGG16 architecture (originally trained on ImageNet) and repurposing them for skin lesion classification on the ISIC 2018 dataset. Specifically, models of VGG16 are tested by exhaustively testing the layers at which weights are extracted from and up to which they are frozen from further training, concluding that extracting all layers from VGG16 and fine-tuning the last two convolutional blocks to the ISIC 2018 dataset is the most performant configuration. However different choices of optimizer and learning rates could unveil better models. For comparison, two custom CNN architectures are explored and trained from scratch in a typical end- to-end learning scheme, from which it can be seen that end-to-end learning of CNN is much harder due to the many different hyperparameters that need to be cross-validated on a wide range of values which is computationally intensive to do thoroughly. In conclusion, transfer learning is a much more practical strategy for skin lesion classification and most other computer vision problems.},
author = {Maia, F{\'{a}}bio and Silva, Filipe and Georgieva, Petia},
institution = {University of Aveiro},
keywords = {binary classification,convolutional neural network,deep learning,medical imaging,skin lesion diagnosis,transfer learning},
title = {A Study of Transfer Learning for Skin Lesion Classification (MSc thesis)},
year = {2019}
}
@article{He2015,
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
journal = {Proceedings of the ieee international conference on computer vision},
pages = {1026--1034},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
year = {2015}
}
@article{Yuan2009,
abstract = {Accurate skin lesion segmentation is critical for automated early skin cancer detection and diagnosis. In this paper, we present a novel multi-modal skin lesion segmentation method based on region fusion and narrow band energy graph partitioning. The proposed method can handle challenging characteristics of skin lesions, such as topological changes, weak or false edges, and asymmetry. Extensive testing demonstrated that in this method complex contours are detected correctly while topological changes of evolving curves are managed naturally. The accuracy of the method was quantified using a lesion similarity measure and lesion segmentation error ratio, Our results were validated using a large set of epiluminescence microscopy (ELM) images acquired using cross-polarization ELM and side-transillumination ELM. Our findings demonstrate that the new method can achieve improved robustness and better overall performance compared to other state-of-the-art segmentation methods.},
author = {Yuan, Xiaojing and Situ, Ning and Zouridakis, George},
doi = {10.1016/j.patcog.2008.09.006},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Active contour,Automatic skin lesion segmentation,Border detection,Melanoma detection,Narrow band energy,Snake},
month = jun,
number = {6},
pages = {1017--1028},
publisher = {Pergamon},
title = {A narrow band graph partitioning method for skin lesion segmentation},
volume = {42},
year = {2009}
}
@misc{vggnet,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
title = {Very Deep Convolutional Networks For Large-Scale Image Recogntinion},
url = {https://arxiv.org/pdf/1409.1556.pdf},
year = {2015}
}
@article{Bissoto2018,
abstract = {This extended abstract describes the participation of RECOD Titans in parts 1 to 3 of the ISIC Challenge 2018 "Skin Lesion Analysis Towards Melanoma Detection" (MICCAI 2018). Although our team has a long experience with melanoma classification and moderate experience with lesion segmentation, the ISIC Challenge 2018 was the very first time we worked on lesion attribute detection. For each task we submitted 3 different ensemble approaches, varying combinations of models and datasets. Our best results on the official testing set, regarding the official metric of each task, were: 0.728 (segmentation), 0.344 (attribute detection) and 0.803 (classification). Those submissions reached, respectively, the 56th, 14th and 9th places.},
archivePrefix = {arXiv},
arxivId = {1808.08480},
author = {Bissoto, Alceu and Perez, F{\'{a}}bio and Ribeiro, Vin{\'{i}}cius and Fornaciali, Michel and Avila, Sandra and Valle, Eduardo},
eprint = {1808.08480},
month = aug,
title = {Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis, Classification: RECOD Titans at ISIC Challenge 2018},
year = {2018}
}
@techreport{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
pages = {249--256},
title = {Understanding the difficulty of training deep feedforward neural networks},
url = {http://proceedings.mlr.press/v9/glorot10a.html},
year = {2010}
}
@article{Barata2014,
abstract = {Melanoma is one of the deadliest forms of cancer; hence, great effort has been put into the development of diagnosis methods for this disease. This paper addresses two different systems for the detection of melanomas in dermoscopy images. The first system uses global methods to classify skin lesions, whereas the second system uses local features and the bag-of-features classifier. This paper aims at determining the best system for skin lesion classification. The other objective is to compare the role of color and texture features in lesion classification and determine which set of features is more discriminative. It is concluded that color features outperform texture features when used alone and that both methods achieve very good results, i.e., Sensitivity = 96{\%} and Specificity = 80{\%} for global methods against Sensitivity = 100{\%} and Specificity = 75{\%} for local methods. The classification results were obtained on a data set of 176 dermoscopy images from Hospital Pedro Hispano, Matosinhos. {\textcopyright} 2014 IEEE.},
author = {Barata, Catarina and Ruela, Margarida and Francisco, Mariana and Mendonca, Teresa and Marques, Jorge S.},
doi = {10.1109/JSYST.2013.2271540},
issn = {19379234},
journal = {IEEE Systems Journal},
keywords = {Bag of features (BoF),color,computer-aided diagnosis,dermoscopy,melanoma,texture},
number = {3},
pages = {965--979},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Two systems for the detection of melanomas in dermoscopy images using texture and color features},
volume = {8},
year = {2014}
}
@article{Vyas2018,
abstract = {As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin {\$}m{\$} between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al.[7] and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.},
archivePrefix = {arXiv},
arxivId = {1809.03576},
author = {Vyas, Apoorv and Jammalamadaka, Nataraj and Zhu, Xia and Das, Dipankar and Kaul, Bharat and Willke, Theodore L.},
eprint = {1809.03576},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Anomaly detection,Out-of-distribution},
month = sep,
pages = {560--574},
publisher = {Springer Verlag},
title = {Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers},
volume = {11212 LNCS},
year = {2018}
}
@article{adagrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
journal = {Journal of Machine Learning Research},
pages = {2121--2159},
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
volume = {12},
year = {2011}
}
@misc{msk,
abstract = {This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classification. Participation involved 593 registrations, 81 pre-submissions, 46 finalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this field to date. While the official challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.},
archivePrefix = {arXiv},
arxivId = {1710.05006},
author = {Codella, Noel C. F.},
eprint = {1710.05006},
month = oct,
title = {Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)},
url = {http://arxiv.org/abs/1710.05006},
year = {2017}
}
@inproceedings{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6572},
month = dec,
publisher = {ICLR},
title = {Explaining and harnessing adversarial examples},
year = {2015}
}
@article{Okur2018,
abstract = {Skin cancer is defined as the rapid growth of skin cells due to DNA damage that cannot be repaired. Melanoma is one of the deadliest types of skin cancer, which originates from melanocytes. While other skin cancer types have limited spreading capabilities, the danger of melanoma comes from its ability to spread (metastasize) rapidly. Fortunately, it can be detected by visual inspection of the skin surface, and it is 100{\%} curable if identified in the early stages. However, detection by “subjective visual inspection” creates an important problem, due to investigators' different levels of experiences and education. Dermoscopy (dermatoscopy) has significantly increased the diagnostic accuracy of melanoma since late 90's. In addition, several systems have been proposed in order to assist investigators or to perform an automatic melanoma detection. This survey focuses on the algorithms for automated melanoma detection in dermoscopic images through an extensive analysis of the stages in methodologies proposed in the literature, and by examining related concepts and describing possible future directions through open problems in this domain of research.},
author = {Okur, Erdem and Turkan, Mehmet},
doi = {10.1016/j.engappai.2018.04.028},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Automated detection,Dermoscopy,Image processing,Machine learning,Melanoma detection,Skin cancer},
month = aug,
pages = {50--67},
publisher = {Elsevier Ltd},
title = {A survey on automated melanoma detection},
volume = {73},
year = {2018}
}
@article{LeCun1989,
author = {LeCun, Yann},
journal = {Connectionism in perspective},
title = {Generalization and network design strategies},
volume = {19},
year = {1989}
}
@article{Shorten2019,
abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
doi = {10.1186/s40537-019-0197-0},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
month = dec,
number = {1},
pages = {60},
publisher = {SpringerOpen},
title = {A survey on Image Data Augmentation for Deep Learning},
volume = {6},
year = {2019}
}
@book{DipanjanSarkarRaghavBali2018,
author = {{Dipanjan Sarkar, Raghav Bali}, Tamoghna Ghosh},
edition = {First},
isbn = {978-1-78883-130-7},
publisher = {Packt Publishing},
title = {Hands-On Transfer Learning with Python},
year = {2018}
}
@article{Fourcade2019,
abstract = {AIM AND SCOPE
Artificial intelligence (AI) in medicine is a fast-growing field. The rise of deep learning algorithms, such as convolutional neural networks (CNNs), offers fascinating perspectives for the automation of medical image analysis. In this systematic review article, we screened the current literature and investigated the following question: “Can deep learning algorithms for image recognition improve visual diagnosis in medicine?” 

MATERIALS AND METHODS
We provide a systematic review of the articles using CNNs for medical image analysis, published in the medical literature before May 2019. Articles were screened based on the following items: type of image analysis approach (detection or classification), algorithm architecture, dataset used, training phase, test, comparison method (with specialists or other), results (accuracy, sensibility and specificity) and conclusion. 

RESULTS
We identified 352 articles in the PubMed database and excluded 327 items for which performance was not assessed (review articles) or for which tasks other than detection or classification, such as segmentation, were assessed. The 25 included papers were published from 2013 to 2019 and were related to a vast array of medical specialties. Authors were mostly from North America and Asia. Large amounts of qualitative medical images were necessary to train the CNNs, often resulting from international collaboration. The most common CNNs such as AlexNet and GoogleNet, designed for the analysis of natural images, proved their applicability to medical images. 

CONCLUSION
CNNs are not replacement solutions for medical doctors, but will contribute to optimize routine tasks and thus have a potential positive impact on our practice. Specialties with a strong visual component such as radiology and pathology will be deeply transformed. Medical practitioners, including surgeons, have a key role to play in the development and implementation of such devices.},
author = {Fourcade, A. and Khonsari, R.H.},
doi = {10.1016/J.JORMAS.2019.06.002},
issn = {2468-7855},
journal = {Journal of Stomatology, Oral and Maxillofacial Surgery},
month = sep,
number = {4},
pages = {279--288},
publisher = {Elsevier Masson},
title = {Deep learning in medical image analysis: A third eye for doctors},
volume = {120},
year = {2019}
}
@article{momentum,
author = {Qian, Ning},
journal = {Neural Networks},
keywords = {dblp},
number = {1},
pages = {145--151},
title = {On the momentum term in gradient descent learning algorithms.},
volume = {12},
year = {1999}
}
@misc{bcn_20000,
abstract = {This article summarizes the BCN20000 dataset, composed of 19424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Cl$\backslash$'inic in Barcelona. With this dataset, we aim to study the problem of unconstrained classification of dermoscopic images of skin cancer, including lesions found in hard-to-diagnose locations (nails and mucosa), large lesions which do not fit in the aperture of the dermoscopy device, and hypo-pigmented lesions. The BCN20000 will be provided to the participants of the ISIC Challenge 2019, where they will be asked to train algorithms to classify dermoscopic images of skin cancer automatically.},
archivePrefix = {arXiv},
arxivId = {1908.02288},
author = {Combalia, Marc and Codella, Noel C. F. and Rotemberg, Veronica and Helba, Brian and Vilaplana, Veronica and Reiter, Ofer and Carrera, Cristina and Barreiro, Alicia and Halpern, Allan C. and Puig, Susana and Malvehy, Josep},
eprint = {1908.02288},
month = aug,
title = {BCN20000: Dermoscopic Lesions in the Wild},
url = {http://arxiv.org/abs/1908.02288},
year = {2019}
}
@article{Greenspan2016,
author = {Greenspan, Hayit and {Van Ginneken}, Bram and Summers, Ronald M.},
doi = {10.1109/TMI.2016.2553401},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
month = may,
number = {5},
pages = {1153--1159},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique},
volume = {35},
year = {2016}
}
@misc{March,
abstract = {Confirming a diagnosis of cutaneous melanoma requires obtaining a skin biopsy specimen. However, obtaining numerous biopsy specimens - which often happens in patients with increased melanoma risk - is associated with significant cost and morbidity. While some melanomas are easily recognized by the naked eye, many can be difficult to distinguish from nevi, and therefore there is a need and opportunity to develop new technologies that can facilitate clinical examination and melanoma diagnosis. In part I of this 2-part continuing medical education article, we will review the practical applications of emerging technologies for noninvasive melanoma diagnosis, including mobile (smartphone) applications, multispectral imaging (ie, MoleMate and MelaFind), and electrical impedance spectroscopy (Nevisense).},
author = {March, Jordon and Hand, Matthew and Grossman, Douglas},
booktitle = {Journal of the American Academy of Dermatology},
doi = {10.1016/j.jaad.2015.02.1138},
issn = {10976787},
keywords = {Mela Find,Mole Mate,Nevisense,melanoma,mobile app,spectroscopy,teledermatology},
month = jun,
number = {6},
pages = {929--941},
publisher = {Mosby Inc.},
title = {Practical application of new technologies for melanoma diagnosis: Part I. Noninvasive approaches},
volume = {72},
year = {2015}
}
@misc{Alom2019,
abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and {Van Essen}, Brian C. and Awwal, Abdul A.S. and Asari, Vijayan K.},
booktitle = {Electronics (Switzerland)},
doi = {10.3390/electronics8030292},
issn = {20799292},
keywords = {Auto-encoder (AE),Convolutional neural network (CNN),Deep belief network (DBN),Deep learning,Deep reinforcement learning (DRL),Generative adversarial network (GAN),Recurrent neural network (RNN),Restricted Boltzmann machine (RBM),Transfer learning},
month = mar,
number = {3},
pages = {292},
publisher = {MDPI AG},
title = {A state-of-the-art survey on deep learning theory and architectures},
volume = {8},
year = {2019}
}
@article{resnetv2,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1603.05027},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
month = mar,
pages = {630--645},
publisher = {Springer Verlag},
title = {Identity Mappings in Deep Residual Networks},
volume = {9908 LNCS},
year = {2016}
}
@article{Mar2018,
author = {Mar, V. J. and Soyer, H. P.},
doi = {10.1093/annonc/mdy193},
issn = {15698041},
journal = {Annals of Oncology},
number = {8},
publisher = {Oxford University Press},
title = {Artificial intelligence for melanoma diagnosis: How can we deliver on the promise?},
volume = {29},
year = {2018}
}
@article{Ching2018,
abstract = {Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recentlyshown impressive results across avarietyof domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood.Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-And discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and Decaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H.S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
doi = {10.1098/rsif.2017.0387},
issn = {17425662},
journal = {Journal of the Royal Society Interface},
number = {141},
pmid = {29618526},
publisher = {Royal Society Publishing},
title = {Opportunities and obstacles for deep learning in biology and medicine},
volume = {15},
year = {2018}
}
@article{Nguyen2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1412.1897},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = dec,
pages = {427--436},
publisher = {IEEE Computer Society},
title = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
volume = {07-12-June},
year = {2014}
}
@misc{Pathan2018,
abstract = {Computerized image analysis methods for dermoscopy are primarily of great interest and benefit, as it provides significant information about the lesion, which can be of pertinence for the clinicians and a stand-alone warning implement. Computer-based diagnostic systems require dedicated image processing algorithms to provide mathematical descriptions of the suspected regions, such systems hold a great potential in oncology. In this paper, we have performed a review of the state of art techniques used in computer-aided diagnostic systems, by giving the domain aspects of melanoma followed by the prominent techniques used in each of the steps. The steps include dermoscopic image pre-processing, segmentation, extraction and selection of peculiar features, and relegation of skin lesions. The paper also presents cognizance to judge the consequentiality of every methodology utilized in the literature, in addition to the corresponding results obtained in this perspective. The inadequacies and the future research directions are accentuated.},
author = {Pathan, Sameena and Prabhu, K. Gopalakrishna and Siddalingaswamy, P. C.},
booktitle = {Biomedical Signal Processing and Control},
doi = {10.1016/j.bspc.2017.07.010},
issn = {17468108},
keywords = {Acquisition,Classification,Dermoscopy,Melanoma,Pigmented skin lesions (PSLs),Segmentation},
month = jan,
pages = {237--262},
publisher = {Elsevier Ltd},
title = {Techniques and algorithms for computer aided diagnosis of pigmented skin lesions—A review},
volume = {39},
year = {2018}
}
@article{Brinker2018,
abstract = {BACKGROUND State-of-the-art classifiers based on convolutional neural networks (CNNs) were shown to classify images of skin cancer on par with dermatologists and could enable lifesaving and fast diagnoses, even outside the hospital via installation of apps on mobile devices. To our knowledge, at present there is no review of the current work in this research area. OBJECTIVE This study presents the first systematic review of the state-of-the-art research on classifying skin lesions with CNNs. We limit our review to skin lesion classifiers. In particular, methods that apply a CNN only for segmentation or for the classification of dermoscopic patterns are not considered here. Furthermore, this study discusses why the comparability of the presented procedures is very difficult and which challenges must be addressed in the future. METHODS We searched the Google Scholar, PubMed, Medline, ScienceDirect, and Web of Science databases for systematic reviews and original research articles published in English. Only papers that reported sufficient scientific proceedings are included in this review. RESULTS We found 13 papers that classified skin lesions using CNNs. In principle, classification methods can be differentiated according to three principles. Approaches that use a CNN already trained by means of another large dataset and then optimize its parameters to the classification of skin lesions are the most common ones used and they display the best performance with the currently available limited datasets. CONCLUSIONS CNNs display a high performance as state-of-the-art skin lesion classifiers. Unfortunately, it is difficult to compare different classification methods because some approaches use nonpublic datasets for training and/or testing, thereby making reproducibility difficult. Future publications should use publicly available benchmarks and fully disclose methods used for training to allow comparability.},
author = {Brinker, Titus Josef and Hekler, Achim and Utikal, Jochen Sven and Grabe, Niels and Schadendorf, Dirk and Klode, Joachim and Berking, Carola and Steeb, Theresa and Enk, Alexander H and von Kalle, Christof},
doi = {10.2196/11936},
issn = {1438-8871},
journal = {Journal of medical Internet research},
keywords = {carcinoma classification,convolutional neural networks,deep learning,lesion classification,melanoma classification,skin cancer},
month = oct,
number = {10},
pages = {e11936},
pmid = {30333097},
publisher = {Journal of Medical Internet Research},
title = {Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review.},
volume = {20},
year = {2018}
}
@misc{Bastien,
abstract = {Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1211.5590v1},
author = {Bastien, Fr{\'{e}}d{\'{e}}ric and {Pascal Lamblin}, Nouizorg and Bergstra, James and Goodfellow, Ian and Bergeron, Arnaud and Bouchard, Nicolas and Warde-Farley, David and Bengio, Yoshua},
eprint = {1211.5590v1},
title = {Theano: new features and speed improvements}
}
@article{isic2019first,
abstract = {In this paper, we describe our method for the ISIC 2019 Skin Lesion Classification Challenge. The challenge comes with two tasks. For task 1, skin lesions have to be classified based on dermoscopic images. For task 2, dermoscopic images and additional patient meta data have to be used. A diverse dataset of 25000 images was provided for training, containing images from eight classes. The final test set contains an additional, unknown class. We address this challenging problem with a simple, data driven approach by including external data with skin lesions types that are not present in the training set. Furthermore, multi-class skin lesion classification comes with the problem of severe class imbalance. We try to overcome this problem by using loss balancing. Also, the dataset contains images with very different resolutions. We take care of this property by considering different model input resolutions and different cropping strategies. To incorporate meta data such as age, anatomical site, and sex, we use an additional dense neural network and fuse its features with the CNN. We aggregate all our models with an ensembling strategy where we search for the optimal subset of models. Our best ensemble achieves a balanced accuracy of 74.2{\%} using five-fold cross-validation. On the official test set our method is ranked first for both tasks with a balanced accuracy of 63.6{\%} for task 1 and 63.4{\%} for task 2.},
archivePrefix = {arXiv},
arxivId = {1910.03910},
author = {Gessert, Nils and Nielsen, Maximilian and Shaikh, Mohsin and Werner, Rene and Schlaefer, Alexander},
eprint = {1910.03910},
journal = {MethodsX},
keywords = {Convolutional neural network,Convolutional neural networks,Deep Learning,Multi-class skin lesion classification},
month = oct,
publisher = {Elsevier B.V.},
title = {Skin Lesion Classification Using Ensembles of Multi-Resolution EfficientNets with Meta Data},
volume = {7},
year = {2019}
}
@article{mixup,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
eprint = {1710.09412},
month = oct,
title = {mixup: Beyond Empirical Risk Minimization},
year = {2017}
}
@misc{ilsvrc,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
booktitle = {International Journal of Computer Vision},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = dec,
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {ImageNet Large Scale Visual Recognition Challenge},
volume = {115},
year = {2015}
}
@article{efficientnet,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {1905.11946},
month = may,
title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
year = {2019}
}
@article{Miotto2017,
abstract = {Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.},
author = {Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and Dudley, Joel T.},
doi = {10.1093/bib/bbx044},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical informatics,Deep learning,Electronic health records,Genomics,Health care,Translational bioinformatics},
month = may,
number = {6},
pages = {1236--1246},
publisher = {Oxford University Press},
title = {Deep learning for healthcare: Review, opportunities and challenges},
volume = {19},
year = {2017}
}
@article{Han2018,
abstract = {We tested the use of a deep learning algorithm to classify the clinical images of 12 skin diseases—basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, actinic keratosis, seborrheic keratosis, malignant melanoma, melanocytic nevus, lentigo, pyogenic granuloma, hemangioma, dermatofibroma, and wart. The convolutional neural network (Microsoft ResNet-152 model; Microsoft Research Asia, Beijing, China) was fine-tuned with images from the training portion of the Asan dataset, MED-NODE dataset, and atlas site images (19,398 images in total). The trained model was validated with the testing portion of the Asan, Hallym and Edinburgh datasets. With the Asan dataset, the area under the curve for the diagnosis of basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, and melanoma was 0.96 ± 0.01, 0.83 ± 0.01, 0.82 ± 0.02, and 0.96 ± 0.00, respectively. With the Edinburgh dataset, the area under the curve for the corresponding diseases was 0.90 ± 0.01, 0.91 ± 0.01, 0.83 ± 0.01, and 0.88 ± 0.01, respectively. With the Hallym dataset, the sensitivity for basal cell carcinoma diagnosis was 87.1{\%} ± 6.0{\%}. The tested algorithm performance with 480 Asan and Edinburgh images was comparable to that of 16 dermatologists. To improve the performance of convolutional neural network, additional images with a broader range of ages and ethnicities should be collected.},
author = {Han, Seung Seog and Kim, Myoung Shin and Lim, Woohyung and Park, Gyeong Hun and Park, Ilwoo and Chang, Sung Eun},
doi = {10.1016/j.jid.2018.01.028},
issn = {15231747},
journal = {Journal of Investigative Dermatology},
month = jul,
number = {7},
pages = {1529--1538},
publisher = {Elsevier B.V.},
title = {Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm},
volume = {138},
year = {2018}
}
@article{Fiore2019,
abstract = {In the last years, the number of frauds in credit card-based online payments has grown dramatically, pushing banks and e-commerce organizations to implement automatic fraud detection systems, performing data mining on huge transaction logs. Machine learning seems to be one of the most promising solutions for spotting illicit transactions, by distinguishing fraudulent and non-fraudulent instances through the use of supervised binary classification systems properly trained from pre-screened sample datasets. However, in such a specific application domain, datasets available for training are strongly imbalanced, with the class of interest considerably less represented than the other. This significantly reduces the effectiveness of binary classifiers, undesirably biasing the results toward the prevailing class, while we are interested in the minority class. Oversampling the minority class has been adopted to alleviate this problem, but this method still has some drawbacks. Generative Adversarial Networks are general, flexible, and powerful generative deep learning models that have achieved success in producing convincingly real-looking images. We trained a GAN to output mimicked minority class examples, which were then merged with training data into an augmented training set so that the effectiveness of a classifier can be improved. Experiments show that a classifier trained on the augmented set outperforms the same classifier trained on the original data, especially as far the sensitivity is concerned, resulting in an effective fraud detection mechanism.},
author = {Fiore, Ugo and Santis, Alfredo and Perla, Francesca and Zanetti, Paolo and Palmieri, Francesco},
doi = {10.1016/j.ins.2017.12.030},
issn = {00200255},
journal = {Information Sciences},
keywords = {Deep learning,Fraud detection,Generative adversarial networks,Supervised classification},
month = apr,
pages = {448--455},
publisher = {Elsevier Inc.},
title = {Using generative adversarial networks for improving classification effectiveness in credit card fraud detection},
volume = {479},
year = {2019}
}
@article{noisystudent,
abstract = {We present a simple self-training method that achieves 88.4{\%} top-1 accuracy on ImageNet, which is 2.0{\%} better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0{\%} to 83.7{\%}, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
archivePrefix = {arXiv},
arxivId = {1911.04252},
author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
eprint = {1911.04252},
month = nov,
title = {Self-training with Noisy Student improves ImageNet classification},
year = {2019}
}
@misc{Zaidan2018,
abstract = {This research aims to review the attempts of researchers in response to the new and disruptive technology of skin cancer applications in terms of evaluation and benchmarking, in order to identify the research landscape from the literature into a cohesive taxonomy. An extensive search was conducted for articles dealing with ‘skin cancer', ‘apps' and ‘smartphone' or ‘mHealth' in different variations to find all the relevant articles in three main databases, namely, “Web of Science”, “Science Direct”, and “IEEE explore”. These databases are considered wide enough to cover medical and technical literature. The final classification scheme outcome of the dataset contained 110 articles that were classified into four classes: development and design; analytical; evaluative and comparative; and review and survey studies. Afterwards, another filtering process was achieved based on the evaluation criteria error rate within the dataset, time complicity and reliability, which are used in skin cancer applications. The final classification scheme outcome of the dataset contained 89 articles distributed in mapping and crossover with four sections concluded from 110 articles. Development and design studies, analytical studies, evaluative and comparative studies and articles of reviews and surveys comprised of 48.3146{\%}, 22.4719{\%}, 16.8539{\%} (15), and 12.3595{\%} (11) of the reviewed articles, respectively. The basic features of this evolving approach were identified in these aspects. We also determined open issues in terms of evaluation and benchmarking that hamper the utility of this technology. Furthermore, with the exception of the 89 papers reviewed, the new recommendation pathway solution was described in order to improve the measurement process for smartphone-based skin cancer diagnosis applications.},
author = {Zaidan, A. A. and Zaidan, B. B. and Albahri, O. S. and Alsalem, M. A. and Albahri, A. S. and Yas, Qahtan M. and Hashim, M.},
booktitle = {Health and Technology},
doi = {10.1007/s12553-018-0223-9},
issn = {21907196},
keywords = {Mobile health,Real-time apps,Skin cancer diagnosis, evaluation and benchmarking},
month = sep,
number = {4},
pages = {223--238},
publisher = {Springer Verlag},
title = {A review on smartphone skin cancer diagnosis apps in evaluation and benchmarking: coherent taxonomy, open issues and recommendation pathway solution},
volume = {8},
year = {2018}
}
@misc{ieeta-review,
abstract = {ieeta-review},
author = {{F. Silva}, P. Georgieva},
keywords = {computer-assisted dermatology,deep learning,machine learning,medical imaging,mobile applications,self-surveillance,skin lesions},
pages = {1--25},
title = {Computer-Assisted Diagnosis of Skin Lesions : An Overview from Self-Surveillance to Deep Learning},
year = {2018}
}
@inproceedings{batchnorm,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
volume = {1},
year = {2015}
}
@article{cutout,
abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56{\%}, 15.20{\%}, and 1.30{\%} test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
archivePrefix = {arXiv},
arxivId = {1708.04552},
author = {DeVries, Terrance and Taylor, Graham W.},
eprint = {1708.04552},
month = aug,
title = {Improved Regularization of Convolutional Neural Networks with Cutout},
year = {2017}
}
@article{Brewer2013,
abstract = {IMPORTANCE: With advancements in mobile technology, cellular phone-based mobile applications (apps) may be used in the practice and delivery of dermatologic care. OBJECTIVE: To identify and categorize the variety of current mobile apps available in dermatology for patients and providers. DESIGN, SETTING, AND PARTICIPANTS: Dermatology-related search terms were queried in the online app stores of the most commonly used mobile platforms developed by Apple, Android, Blackberry, Nokia, and Windows. Applications were assigned to categories based on description. Popularity, price, and reviews were recorded and target audiences were determined through websites offering online mobile apps. MAIN OUTCOMES AND MEASURES: Number, type, and price of mobile apps in dermatology. RESULTS: A total of 229 dermatology-related apps were identified in the following categories: general dermatology reference (61 [26.6{\%}]), self-surveillance/diagnosis (41 [17.9{\%}]), disease guide (39 [17.0{\%}]), educational aid (20 [8.7{\%}]), sunscreen/UV recommendation (19 [8.3{\%}]), calculator (12 [5.2{\%}]), teledermatology (8 [3.5{\%}]), conference (6 [2.6{\%}]), journal (6 [2.6{\%}]), photograph storage/sharing (5 [2.2{\%}]), dermoscopy (2 [0.9{\%}]), pathology (2 [0.9{\%}]), and other (8 [3.5{\%}]). The most reviewed apps included Ultraviolet ∼ UV Index (355 reviews), VisualDx (306), SPF (128), iSore (61), and SpotMole (50). There were 209 unique apps, with 17 apps existing on more than 1 operating system. More than half of the apps were offered free of charge (117 [51.1{\%}]). Paid apps (112 [48.9{\%}]) ranged from {\$}0.99 to {\$}139.99 (median, {\$}2.99). Target audiences included patient (117 [51.1{\%}]), health care provider (94 [41.0{\%}]), and both (18 [7.9{\%}]). CONCLUSIONS AND RELEVANCE: The widespread variety and popularity of mobile apps demonstrate a great potential to expand the practice and delivery of dermatologic care. Copyright 2013 American Medical Association. All rights reserved.},
author = {Brewer, Ann Chang and Endly, Dawnielle C. and Henley, Jill and Amir, Mahsa and Sampson, Blake P. and Moreau, Jacqueline F. and Dellavalle, Robert P.},
doi = {10.1001/jamadermatol.2013.5517},
issn = {21686068},
journal = {JAMA Dermatology},
month = nov,
number = {11},
pages = {1300--1304},
publisher = {JAMA Dermatol},
title = {Mobile applications in dermatology},
volume = {149},
year = {2013}
}
@misc{isic2018,
title = {International Skin Imaging Collaboration (ISIC) Challenge 2018 Task 3: Lesion Diagnosis},
url = {https://challenge2018.isic-archive.com/task3/},
urldate = {2019-11-23},
year = {2018}
}
@article{horizontalvertical,
abstract = {Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.},
archivePrefix = {arXiv},
arxivId = {1306.2759},
author = {Xie, Jingjing and Xu, Bing and Chuang, Zhang},
eprint = {1306.2759},
month = jun,
title = {Horizontal and Vertical Ensemble with Deep Representation for Classification},
year = {2013}
}
@article{Krizhevskya2,
abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.},
author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
keywords = {Dataset},
title = {CIFAR-10 (Canadian Institute for Advanced Research)},
}
@article{Tang2013,
abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these "deep learning" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1306.0239},
author = {Tang, Yichuan},
eprint = {1306.0239},
month = jun,
title = {Deep Learning using Linear Support Vector Machines},
year = {2013}
}
@article{Miller2018,
abstract = {Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society—forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100{\%}, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials.},
author = {Miller, D. Douglas and Brown, Eric W.},
doi = {10.1016/J.AMJMED.2017.10.035},
issn = {0002-9343},
journal = {The American Journal of Medicine},
month = feb,
number = {2},
pages = {129--133},
publisher = {Elsevier},
title = {Artificial Intelligence in Medical Practice: The Question to the Answer?},
volume = {131},
year = {2018}
}
@article{Engineeringa,
author = {Engineering, Audiovisual Systems},
number = {January 2017},
title = {SKIN LESION DETECTION FROM DERMOSCOPIC IMAGES USING CONVOLUTIONAL NEURAL NETWORKS Submitted to the Faculty of the Escola T ` o de Barcelona a Romero L ´ opez In partial fulfillment of the requirements for the degree in Audiovisual Systems Engineering Advi}
}
@article{Sensoy2018,
abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.},
archivePrefix = {arXiv},
arxivId = {1806.01768},
author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
eprint = {1806.01768},
journal = {Advances in Neural Information Processing Systems},
month = jun,
pages = {3179--3189},
publisher = {Neural information processing systems foundation},
title = {Evidential Deep Learning to Quantify Classification Uncertainty},
volume = {2018-Decem},
year = {2018}
}
@article{Zech2018,
abstract = {Background: There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings: A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5{\%}, 44.8{\%}, and 57.3{\%}, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong's test. The prevalence of pneumonia was high enough at MSH (34.2{\%}) relative to NIH and IU (1.2{\%} and 1.0{\%}) that merely sorting by hospital system achieved an AUC of 0.861 (95{\%} CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both {\textless}0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95{\%} CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95{\%} CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P {\textless} 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P {\textless} 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95{\%} NIH (22,050/22,062) and 99.98{\%} MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases. Conclusion: Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.},
author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
doi = {10.1371/journal.pmed.1002683},
editor = {Sheikh, Aziz},
issn = {1549-1676},
journal = {PLOS Medicine},
month = nov,
number = {11},
pages = {e1002683},
publisher = {Public Library of Science},
title = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study},
volume = {15},
year = {2018}
}
@misc{ieeta-collaboration,
author = {Silva, Filipe and Georgieva, P{\'{e}}tia},
title = {Machine Learning for Automated Diagnosis of Pigmented Skin Lesions},
year = {2018}
}
@book{Nielsen2017a,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
pages = {389--411},
title = {Neural Networks and Deep Learning},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2018}
}
@phdthesis{marques2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Marques, Jo{\~{a}}o},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
school = {University of Aveiro},
title = {A Self-Surveillance System for Change Detection of Pigmented Skin Lesions},
year = {2019}
}
@techreport{isic2019third,
author = {Pollastri, Federico and Maro{\~{n}}as, Juan and Parre{\~{n}}o, Mario and Bolelli, Federico and Paredes, Roberto and Grana, Costantino and Albiol, Alberto},
institution = {AImageLab},
title = {AImageLab-PRHLT at ISIC Challenge 2019},
year = {2019}
}
@misc{Cs231n,
title = {Cs231n convolutional neural networks for visual recognition},
url = {https://cs231n.github.io/ convolutional-networks/},
urldate = {2020-01-22}
}
@article{Unlu2014,
abstract = {Dermatoscopic analysis of melanocytic lesions using the CASH algorithm has rarely been described in the literature. The purpose of this study was to compare the sensitivity, specificity, and diagnostic accuracy rates of the ABCD rule of dermatoscopy, the seven-point checklist, the three-point checklist, and the CASH algorithm in the diagnosis and dermatoscopic evaluation of melanocytic lesions on the hairy skin. One hundred and fifteen melanocytic lesions of 115 patients were examined retrospectively using dermatoscopic images and compared with the histopathologic diagnosis. Four dermatoscopic algorithms were carried out for all lesions. The ABCD rule of dermatoscopy showed sensitivity of 91.6{\%}, specificity of 60.4{\%}, and diagnostic accuracy of 66.9{\%}. The seven-point checklist showed sensitivity, specificity, and diagnostic accuracy of 87.5, 65.9, and 70.4{\%}, respectively; the three-point checklist 79.1, 62.6, 66{\%}; and the CASH algorithm 91.6, 64.8, and 70.4{\%}, respectively. To our knowledge, this is the first study that compares the sensitivity, specificity and diagnostic accuracy of the ABCD rule of dermatoscopy, the three-point checklist, the seven-point checklist, and the CASH algorithm for the diagnosis of melanocytic lesions on the hairy skin. In our study, the ABCD rule of dermatoscopy and the CASH algorithm showed the highest sensitivity for the diagnosis of melanoma. {\textcopyright} 2014 Japanese Dermatological Association.},
author = {Unlu, Ezgi and Akay, Bengu N. and Erdem, Cengizhan},
doi = {10.1111/1346-8138.12491},
issn = {13468138},
journal = {Journal of Dermatology},
keywords = {ABCD rule of dermatoscopy,CASH algorithm,dermatoscopy,seven-point checklist,three-point checklist},
number = {7},
pages = {598--603},
publisher = {Blackwell Publishing Ltd},
title = {Comparison of dermatoscopic diagnostic algorithms based on calculation: The ABCD rule of dermatoscopy, the seven-point checklist, the three-point checklist and the CASH algorithm in dermatoscopic evaluation of melanocytic lesions},
volume = {41},
year = {2014}
}
@article{shadesgray,
author = {{Finlayson, Graham and Trezzi}, Elisabetta},
journal = {Proceedings of the 12th Color Imaging Conference},
pages = {37--41},
title = {Shades of Gray and Colour Constancy},
year = {2004}
}
@article{Kassianos2015,
abstract = {Smartphone health applications ('apps') are widely available but experts remain cautious about their utility and safety. We reviewed currently available apps for the detection of melanoma (July 2014), aimed at general community, patient and generalist clinician users. A proforma was used to extract and assess each app that met the inclusion criteria, and we undertook content analysis to evaluate their content and the evidence applied in their development. Thirty-nine apps were identified with the majority available only for Apple users. Over half (n = 22) provided information or education about melanoma, ultraviolet radiation exposure prevention advice, and skin self-examination strategies, mainly using the ABCDE (A, Asymmetry; B, Border; C, Colour; D, Diameter; E, Evolving) method. Half (n = 19) helped users take and store images of their skin lesions either for review by a dermatologist or for self-monitoring to identify change, an important predictor of melanoma; a similar number (n = 18) used reminders to help users monitor their skin lesions. A few (n = 9) offered expert review of images. Four apps provided a risk assessment to patients about the probability that a lesion was malignant or benign, and one app calculated users' future risk of melanoma. None of the apps appeared to have been validated for diagnostic accuracy or utility using established research methods. Smartphone apps for detecting melanoma by nonspecialist users have a range of functions including information, education, classification, risk assessment and monitoring change. Despite their potential usefulness, and while clinicians may choose to use apps that provide information to educate their patients, apps for melanoma detection require further validation of their utility and safety. What's already known about this topic? Earlier detection of melanoma would allow timely treatment and could improve outcomes. Although smartphone applications ('apps') are recognized as having potentially wide use in dermatology and oncology, experts have expressed caution concerning their diagnostic utility and safety. What does this study add? We identified almost 40 smartphone apps available to detect or prevent melanoma by nonspecialist users including previously unaffected individuals, patients previously diagnosed with skin cancer, and generalist clinicians. Most apps gave advice or education about melanoma, ultraviolet radiation exposure preventive advice, and skin self-examination strategies; half of the apps enabled patients to capture and store images of their skin lesions either for review by a dermatologist or for self-monitoring to identify change, an important predictor of melanoma; only four apps provided a risk assessment about a skin lesion. There was little evidence of clinical or research-based input into the design of these apps or of evaluation of their utility, so clinicians should be cautious about supporting the use of such apps to detect melanoma.},
author = {Kassianos, A. P. and Emery, J. D. and Murchie, P. and Walter, F. M.},
doi = {10.1111/bjd.13665},
issn = {13652133},
journal = {British Journal of Dermatology},
month = jun,
number = {6},
pages = {1507--1518},
pmid = {25600815},
publisher = {Blackwell Publishing Ltd},
title = {Smartphone applications for melanoma detection by community, patient and generalist clinician users: A review},
volume = {172},
year = {2015}
}
@article{Lucieri2020a,
abstract = {Deep learning based medical image classifiers have shown remarkable prowess in various application areas like ophthalmology, dermatology, pathology, and radiology. However, the acceptance of these Computer-Aided Diagnosis (CAD) systems in real clinical setups is severely limited primarily because their decision-making process remains largely obscure. This work aims at elucidating a deep learning based medical image classifier by verifying that the model learns and utilizes similar disease-related concepts as described and employed by dermatologists. We used a well-trained and high performing neural network developed by REasoning for COmplex Data (RECOD) Lab for classification of three skin tumours, i.e. Melanocytic Naevi, Melanoma and Seborrheic Keratosis and performed a detailed analysis on its latent space. Two well established and publicly available skin disease datasets, PH2 and derm7pt, are used for experimentation. Human understandable concepts are mapped to RECOD image classification model with the help of Concept Activation Vectors (CAVs), introducing a novel training and significance testing paradigm for CAVs. Our results on an independent evaluation set clearly shows that the classifier learns and encodes human understandable concepts in its latent representation. Additionally, TCAV scores (Testing with CAVs) suggest that the neural network indeed makes use of disease-related concepts in the correct way when making predictions. We anticipate that this work can not only increase confidence of medical practitioners on CAD but also serve as a stepping stone for further development of CAV-based neural network interpretation methods.},
archivePrefix = {arXiv},
arxivId = {2005.02000},
author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Braun, Stephan Alexander and Malik, Muhammad Imran and Dengel, Andreas and Ahmed, Sheraz},
eprint = {2005.02000},
month = may,
title = {On Interpretability of Deep Learning based Skin Lesion Classifiers using Concept Activation Vectors},
year = {2020}
}
@inproceedings{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
keywords = {adam},
month = dec,
publisher = {ICLR},
title = {Adam: A method for stochastic optimization},
year = {2015}
}
@article{Nachbar1994,
abstract = {Background: The difficulties in accurately assessing pigmented skin lesions are ever present in practice. The recently described ABCD rule of dermatoscopy (skin surface microscopy at ×10 magnification), based on the criteria asymmetry (A), border (B), color (C), and differential structure (D), improved diagnostic accuracy when applied retrospectively to clinical slides. Objective: A study was designed to evaluate the prospective value of the ABCD rule of dermatoscopy in melanocytic lesions. Methods: In 172 melanocytic pigmented skin lesions, the criteria of the ABCD rule of dermatoscopy were analyzed with a semiquantitative scoring system before excision. Results: According to the retrospectively determined threshold, tumors with a score higher than 5.45 (64/69 melanomas [92.8{\%}]) were classified as malignant, whereas lesions with a lower score were considered as benign (93/103 melanocytic nevi [90.3{\%}]). Negative predictive value for melanoma (True-Negative ÷ [True-Negative + False-Negative]) was 9 5.8{\%}, whereas positive predictive value (True-Positive ÷ [True-Positive + False-Positive]) was 85.3{\%}. Diagnostic accuracy for melanoma (True-Positive ÷ [True-Positive + False- Positive + False-Negative]) was 80.0{\%}, compared with 64.4{\%} by the naked eye. Melanoma showed a mean final dermatoscopy score of 6.79 (SD, ± 0.92), significantly differing from melanocytic nevi (mean score, 4.27 ± 0.99; p {\textless}0.01, U test). Conclusion: The ABCD rule can be easily learned and rapidly calculated, and has proven to be reliable. It should be routinely applied to all equivocal pigmented skin lesions to reach a more objective and reproducible diagnosis and to obtain this assessment preoperatively. {\textcopyright} 1994, American Academy of Dermatology, Inc.. All rights reserved.},
author = {Nachbar, Franz and Stolz, Wilhelm and Merkle, Tanja and Cognetta, Armand B. and Vogt, Thomas and Landthaler, Michael and Bilek, Peter and Braun-Falco, Otto and Plewig, Gerd},
doi = {10.1016/S0190-9622(94)70061-3},
issn = {01909622},
journal = {Journal of the American Academy of Dermatology},
month = apr,
number = {4},
pages = {551--559},
pmid = {8157780},
publisher = {Mosby},
title = {The ABCD rule of dermatoscopy: High prospective value in the diagnosis of doubtful melanocytic skin lesions},
volume = {30},
year = {1994}
}
@article{Pehamberger1987,
abstract = {The importance of recognizing early melanoma is generally accepted. Because not all pigmented skin lesions can be diagnosed correctly by their clinical appearance, additional criteria are required for the clinical diagnosis of such lesions. In vivo epiluminescence microscopy provides for a more detailed inspection of the surface of pigmented skin lesions, and, by using the oil immersion technic, which renders the epidermis translucent, opens a new dimension of skin morphology by including the dermoepidermal junction into the macroscopic evaluation of a lesion. In an epiluminescence microscopy study of more than 3000 pigmented skin lesions we have defined morphologic criteria that are not readily apparent to the naked eye but that are detected easily by epiluminescence microscopy and represent relatively reliable markers of benign and malignant pigmented skin lesions. These features include specific patterns, colors, and intensities of pigmentation, as well as the configuration, regularity, and other characteristics of both the margin and the surface of pigmented skin lesions. Pattern analysis of these features permits a distinction between different types of pigmented skin lesions and, in particular, between benign and malignant growth patterns. Epiluminescence microscopy is thus a valuable addition to the diagnostic armamentarium of pigmented skin lesions at a clinical level. {\textcopyright} 1987, American Academy of Dermatology, Inc.. All rights reserved.},
author = {Pehamberger, Hubert and Steiner, Andreas and Wolff, Klaus},
doi = {10.1016/S0190-9622(87)70239-4},
issn = {01909622},
journal = {Journal of the American Academy of Dermatology},
month = oct,
number = {4},
pages = {571--583},
pmid = {3668002},
publisher = {Mosby},
title = {In vivo epiluminescence microscopy of pigmented skin lesions. I. Pattern analysis of pigmented skin lesions},
volume = {17},
year = {1987}
}
@article{sgdr,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1608.03983},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = aug,
publisher = {ICLR},
title = {SGDR: Stochastic Gradient Descent with Warm Restarts},
year = {2016}
}
@article{Walter2013,
abstract = {Background GPs need to recognise significant pigmented skin lesions, given rising UK incidence rates for malignant melanoma. The 7-point checklist (7PCL) has been recommended by NICE (2005) for routine use in UK general practice to identify clinically significant lesions which require urgent referral. Aim To validate the Original and Weighted versions of the 7PCL in the primary care setting. Design and setting Diagnostic validation study, using data from a SIAscopic diagnostic aid randomised controlled trial in eastern England. Method Adults presenting in general practice with a pigmented skin lesion that could not be immediately diagnosed as benign were recruited into the trial. Reference standard diagnoses were histology or dermatology expert opinion; 7PCL scores were calculated blinded to the reference diagnosis. A case was defined as a clinically significant lesion for primary care referral to secondary care (total 1436 lesions: 225 cases, 1211 controls); or melanoma (36). Results For diagnosing clinically significant lesions there was a difference between the performance of the Original and Weighted 7PCLs (respectively, area under curve: 0.66, 0.69, difference = 0.03, P{\textless}0.001). For the identification of melanoma, similar differences were found. Increasing the Weighted 7PCL's cut-off score from recommended 3 to 4 improved detection of clinically significant lesions in primary care: sensitivity 73.3{\%}, specificity 57.1{\%}, positive predictive value 24.1{\%}, negative predictive value 92.0{\%}, while maintaining high sensitivity of 91.7{\%} and moderate specificity of 53.4{\%} for melanoma. Conclusion The Original and Weighted 7PCLs both performed well in a primary care setting to identify clinically significant lesions as well as melanoma. The Weighted 7PCL, with a revised cut-off score of 4 from 3, performs slightly better and could be applied in general practice to support the recognition of clinically significant lesions and therefore the early identification of melanoma. {\textcopyright} British Journal of General Practice.},
author = {Walter, Fiona M. and Prevost, A. Toby and Vasconcelos, Joana and Hall, Per N. and Burrows, Nigel P. and Morris, Helen C. and Kinmonth, Ann Louise and Emery, Jon D.},
doi = {10.3399/bjgp13X667213},
issn = {09601643},
journal = {British Journal of General Practice},
keywords = {Diagnostic techniques and procedures,General practice,Melanoma,Pigmented skin lesions},
number = {610},
publisher = {Royal College of General Practitioners},
title = {Using the 7-point checklist as a diagnostic aid for pigmented skin lesions in general practice: A diagnostic validation study},
volume = {63},
year = {2013}
}
@article{Ng,
author = {Ng, Andrew},
doi = {10.1145/1015330.1015435},
journal = {Proceedings of the Twenty-First International Conference on Machine Learning},
title = {Feature selection, L1 vs. L2 regularization, and rotational invariance},
year = {2004}
}
@inproceedings{Pham2018,
abstract = {Deep CNN techniques have dramatically become the state of the art in image classification. However, applying high-capacity Deep CNN in medical image analysis has been impeded because of scarcity of labeled data. This study has two primary contributions: first, we propose a classification model to improve performance of classification of skin lesion using Deep CNN and Data Augmentation. Second, we demonstrate the use of image data augmentation for overcoming the problem of data limitation and examine the influence of different number of augmented samples on the performance of different classifiers. The proposed classification system is evaluated using the largest public skin lesion testing dataset, containing 600 testing images, and 6,162 training images. New state-of-the-art performance result is archived with AUC (89.2{\%} vs. 87.4{\%}), AP (73.9{\%} vs. 71.5{\%}), and ACC (89.0{\%} vs. 87.2{\%}). In additional, we explore the influence of each image augmentation on the three classifiers and observe that performance of each classifier is influenced differently by each augmentation and has better results comparing with traditional methods. Thus, it is suggested that the performance of skin cancer classification and medial image classification could be improved further by applying data augmentation.},
author = {Pham, Tri Cong and Luong, Chi Mai and Visani, Muriel and Hoang, Van Dung},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-75420-8_54},
isbn = {9783319754192},
issn = {16113349},
keywords = {Data augmentation,Deep learning,Medical image,Melanoma classification,Skin cancer},
pages = {573--582},
publisher = {Springer Verlag},
title = {Deep CNN and Data Augmentation for Skin Lesion Classification},
volume = {10752 LNAI},
year = {2018}
}
@misc{isic2019,
title = {International Skin Imaging Collaboration (ISIC) Challenge 2019},
url = {https://challenge2019.isic-archive.com/},
urldate = {2019-11-23},
year = {2019}
}
@article{densenet,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
eprint = {1608.06993},
month = aug,
title = {Densely Connected Convolutional Networks},
year = {2016}
}
@techreport{isic2019fifth,
author = {Chouhan, Vikas},
title = {Skin Lesion Analysis towards Melanoma Detection with Deep Convolutional Neural Network},
year = {2019}
}
@misc{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2017.07.005},
issn = {13618423},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
month = dec,
pages = {60--88},
pmid = {28778026},
publisher = {Elsevier B.V.},
title = {A survey on deep learning in medical image analysis},
volume = {42},
year = {2017}
}
@misc{archive,
title = {The international skin imaging collaboration: Melanoma project},
url = {https://www.isic-archive.com/{\#}},
urldate = {2020-07-08}
}
@article{Hu2018,
abstract = {In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a summary and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.},
author = {Hu, Zilong and Tang, Jinshan and Wang, Ziming and Zhang, Kai and Zhang, Lin and Sun, Qingling},
doi = {10.1016/j.patcog.2018.05.014},
issn = {00313203},
journal = {Pattern Recognition},
month = nov,
pages = {134--149},
publisher = {Elsevier Ltd},
title = {Deep learning for image-based cancer detection and diagnosis - A survey},
volume = {83},
year = {2018}
}
@article{Abbas2011,
abstract = {Background/purpose: Automated border detection is an important and challenging task in the computerized analysis of dermoscopy images. However, dermoscopic images often contain artifacts such as illumination, dermoscopic gel, and outline (hair, skin lines, ruler markings, and blood vessels). As a result, there is a need for robust methods to remove artifacts and detect lesion borders in dermoscopy images.Methods: This automated method consists of three main steps: (1) preprocessing, (2) edge candidate point detection, and (3) tumor outline delineation. First, algorithms to reduce artifacts were used. Second, a least-squares method (LSM) was performed to acquire edge points. Third, dynamic programming (DP) technique was used to find the optimal boundary of the lesion. Statistical measures based on dermatologist-drawn borders were utilized as ground-truth to evaluate the performance of the proposed method.Results: The method is tested on a total of 240 dermoscopic images: 30 benign melanocytic, 50 malignant melanomas, 50 basal cell carcinomas, 20 Merkel cell carcinomas, 60 seborrheic keratosis, and 30 atypical naevi. We obtained mean border detection error of 8.6{\%}, 5.04{\%}, 9.0{\%}, 7.02{\%}, 2.01{\%}, and 3.24{\%}, respectively.Conclusions: The results demonstrate that border detection combined with artifact removal increases sensitivity and specificity for segmentation of lesions in dermoscopy images. {\textcopyright} 2011 John Wiley {\&} Sons A/S.},
author = {Abbas, Qaisar and Celebi, M. Emre and {Fond{\'{o}}n Garc{\'{i}}a}, Irene and Rashid, Muhammad},
doi = {10.1111/j.1600-0846.2010.00472.x},
issn = {0909752X},
journal = {Skin Research and Technology},
keywords = {Artifacts removal,Border detection,Dermoscopy,Dynamic Programming,Skin cancer},
month = feb,
number = {1},
pages = {91--100},
publisher = {Skin Res Technol},
title = {Lesion border detection in dermoscopy images using dynamic programming},
volume = {17},
year = {2011}
}
@techreport{pytorch,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
title = {Automatic differentiation in PyTorch},
year = {2017}
}
@misc{ftc,
month = feb,
title = {Federal Trade Commission Cracks Down on Marketers of Melanoma Detection Apps},
url = {https://www.ftc.gov/news-events/press-releases/2015/02/ftc-cracks-down-marketers-melanoma-detection-apps},
urldate = {2020-07-02},
year = {2015}
}
@inproceedings{Ali2012,
abstract = {Malignant melanoma is the third most frequent type of skin cancer and one of the most malignant tumors, accounting for 79{\%} of skin cancer deaths. Melanoma is highly curable if diagnosed early and treated properly as survival rate varies between 15{\%} and 65{\%} from early to terminal stages, respectively. So far, melanoma diagnosis is depending subjectively on the dermatologist's expertise. Computer-aided diagnosis (CAD) systems based on epiluminescense light microscopy can provide an objective second opinion on pigmented skin lesions (PSL). This work systematically analyzes the evidence of the effectiveness of automated melanoma detection in images from a dermatoscopic device. Automated CAD applications were analyzed to estimate their diagnostic outcome. Searching online databases for publication dates between 1985 and 2011, a total of 182 studies on dermatoscopic CAD were found. With respect to the systematic selection criterions, 9 studies were included, published between 2002 and 2011. Those studies formed databases of 14,421 dermatoscopic images including both malignant "melanoma" and benign "nevus", with 8,110 images being available ranging in resolution from 150 x 150 to 1568 x 1045 pixels. Maximum and minimum of sensitivity and specificity are 100.0{\%} and 80.0{\%} as well as 98.14{\%} and 61.6{\%}, respectively. Area under the receiver operator characteristics (AUC) and pooled sensitivity, specificity and diagnostics odds ratio are respectively 0.87, 0.90, 0.81, and 15.89. So, although that automated melanoma detection showed good accuracy in terms of sensitivity, specificity, and AUC, but diagnostic performance in terms of DOR was found to be poor. This might be due to the lack of dermatoscopic image resources (ground truth) that are needed for comprehensive assessment of diagnostic performance. In future work, we aim at testing this hypothesis by joining dermatoscopic images into a unified database that serves as a standard reference for dermatology related research in PSL classification.},
author = {Ali, Abder-Rahman A. and Deserno, Thomas M.},
booktitle = {Medical Imaging 2012: Image Perception, Observer Performance, and Technology Assessment},
doi = {10.1117/12.912389},
isbn = {9780819489678},
issn = {16057422},
month = feb,
pages = {83181I},
publisher = {SPIE},
title = {A systematic review of automated melanoma detection in dermatoscopic images and its ground truth data},
volume = {8318},
year = {2012}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
month = feb,
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
volume = {1},
year = {2015}
}
@misc{Haenssle2018,
abstract = {Background: Deep learning convolutional neural networks (CNN) May facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking. Methods: Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge. Results: In level-I dermatologists achieved a mean (6standard deviation) sensitivity and specificity for lesion classification of 86.6{\%} (69.3{\%}) and 71.3{\%} (611.2{\%}), respectively. More clinical information (level-II) improved the sensitivity to 88.9{\%} (69.6{\%}, P ¼ 0.19) and specificity to 75.7{\%} (611.7{\%}, P {\textless} 0.05). The CNN ROC curve revealed a higher specificity of 82.5{\%} when compared with dermatologists in level-I (71.3{\%}, P {\textless} 0.01) and level-II (75.7{\%}, P {\textless} 0.01) at their sensitivities of 86.6{\%} and 88.9{\%}, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P {\textless} 0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge. Conclusions: For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they May benefit from assistance by a CNN's image classification.},
author = {Haenssle, H. A. and Fink, C. and Schneiderbauer, R. and Toberer, F. and Buhl, T. and Blum, A. and Kalloo, A. and {Ben Hadj Hassen}, A. and Thomas, L. and Enk, A. and Uhlmann, L.},
booktitle = {Annals of Oncology},
doi = {10.1093/annonc/mdy166},
issn = {15698041},
keywords = {Automated melanoma detection,Computer algorithm,Deep learning convolutional neural network,Dermoscopy,Melanocytic nevi,Melanoma},
number = {8},
pages = {1836--1842},
publisher = {Oxford University Press},
title = {Man against Machine: Diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists},
volume = {29},
year = {2018}
}
@article{alexnet,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
issn = {15577317},
journal = {Communications of the ACM},
month = jun,
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {ImageNet classification with deep convolutional neural networks},
volume = {60},
year = {2012}
}
@misc{Celebi2019,
abstract = {Dermoscopy is a non-invasive skin imaging technique that permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. While studies on the automated analysis of dermoscopy images date back to the late 1990s, because of various factors (lack of publicly available datasets, open-source software, computational power, etc.), the field progressed rather slowly in its first two decades. With the release of a large public dataset by the International Skin Imaging Collaboration in 2016, development of open-source software for convolutional neural networks, and the availability of inexpensive graphics processing units, dermoscopy image analysis has recently become a very active research field. In this paper, we present a brief overview of this exciting subfield of medical image analysis, primarily focusing on three aspects of it, namely, segmentation, feature extraction, and classification. We then provide future directions for researchers.},
author = {Celebi, M. Emre and Codella, Noel and Halpern, Allan},
booktitle = {IEEE Journal of Biomedical and Health Informatics},
doi = {10.1109/JBHI.2019.2895803},
issn = {21682194},
keywords = {Skin cancer,computer-aided diagnosis,dermoscopy,dermoscopy image analysis,melanoma},
month = mar,
number = {2},
pages = {474--478},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Dermoscopy Image Analysis: Overview and Future Directions},
volume = {23},
year = {2019}
}
@inproceedings{VanMolle2018,
abstract = {Because of their state-of-the-art performance in computer vision, CNNs are becoming increasingly popular in a variety of fields, including medicine. However, as neural networks are black box function approximators, it is difficult, if not impossible, for a medical expert to reason about their output. This could potentially result in the expert distrusting the network when he or she does not agree with its output. In such a case, explaining why the CNN makes a certain decision becomes valuable information. In this paper, we try to open the black box of the CNN by inspecting and visualizing the learned feature maps, in the field of dermatology. We show that, to some extent, CNNs focus on features similar to those used by dermatologists to make a diagnosis. However, more research is required for fully explaining their output.},
archivePrefix = {arXiv},
arxivId = {1809.03851},
author = {{Van Molle}, Pieter and {De Strooper}, Miguel and Verbelen, Tim and Vankeirsbilck, Bert and Simoens, Pieter and Dhoedt, Bart},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-02628-8_13},
eprint = {1809.03851},
isbn = {9783030026271},
issn = {16113349},
keywords = {Deep learning,Dermatology,Skin lesions,Visualization},
month = sep,
pages = {115--123},
publisher = {Springer Verlag},
title = {Visualizing convolutional neural networks to improve decision support for skin lesion classification},
volume = {11038 LNCS},
year = {2018}
}
@article{Johr2002,
author = {Johr, Robert H.},
doi = {10.1016/S0738-081X(02)00236-5},
issn = {0738081X},
journal = {Clinics in Dermatology},
number = {3},
pages = {240--247},
publisher = {Clin Dermatol},
title = {Dermoscopy: Alternative melanocytic algorithms - The ABCD rule of dermatoscopy, menzies scoring method, and 7-point checklist},
volume = {20},
year = {2002}
}
@article{Zhou2013,
abstract = {In recent years, gradient vector flow (GVF) based algorithms have been successfully used to segment a variety of 2-D and 3-D imagery. However, due to the compromise of internal and external energy forces within the resulting partial differential equations, these methods may lead to biased segmentation results. In this paper, we propose MSGVF, a mean shift based GVF segmentation algorithm that can successfully locate the correct borders. MSGVF is developed so that when the contour reaches equilibrium, the various forces resulting from the different energy terms are balanced. In addition, the smoothness constraint of image pixels is kept so that over- or under-segmentation can be reduced. Experimental results on publicly accessible datasets of dermoscopic and optic disc images demonstrate that the proposed method effectively detects the borders of the objects of interest. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Zhou, Huiyu and Li, Xuelong and Schaefer, Gerald and Celebi, M. Emre and Miller, Paul},
doi = {10.1016/j.cviu.2012.11.015},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Contour,Energy function,Gradient vector flow,Image segmentation,Mean shift},
month = sep,
number = {9},
pages = {1004--1016},
publisher = {Academic Press Inc.},
title = {Mean shift based gradient vector flow for image segmentation},
volume = {117},
year = {2013}
}
@techreport{Wang,
abstract = {This manuscript presents the approach used for ISIC 2019 Challenge Task 1: classify dermoscopic images among nine different diagnostic categories without meta-data. This approach consists of an ensemble of convolutional neural networks for classifying 8 known categories and an out-of-distribution detector to determine whether an image belongs to the unknown category. An extra out-of-distribution dataset containing 134 dermoscopic images was used to train the parameters of the detector. Unfortunately, the detector performed poorly in distinguishing in-and out-distribution samples. Only about 16{\%} of out-distribution samples were classified correctly. Consequently, the balanced multi-class accuracy was dropped from 0.838 to 0.754 for 8-and 9-category classification respectively.},
author = {Wang, Hsin-Wei},
title = {Skin Lesion Classification using Ensemble of Convolutional Neural Networks and Out-of-Distribution Detector},
url = {https://github.com/wanghsinwei/isic-2019},
year = {2019}
}
@article{Mete2011,
abstract = {Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Automated assessment tools for dermoscopy images have become an important research field mainly because of inter- and intra-observer variations in human interpretation. One of the most important steps in dermoscopy image analysis is automated detection of lesion borders. In this study, we introduce a border-driven density-based framework to identify skin lesion(s) in dermoscopy images. Unlike the conventional density-based clustering algorithms, proposed algorithm expands regions only at borders of a cluster that in turn speeds up the process without losing precision or recall. In our method, border regions are represented with one or more simple polygons at any time. We tested our algorithm on a dataset of 100 dermoscopy cases with multiple physicians' drawn ground truth borders. The results show that border error and f-measure of assessment averages out at 6.9{\%} and 0.86 respectively. {\textcopyright} 2010 Elsevier Ltd.},
author = {Mete, Mutlu and Kockara, Sinan and Aydin, Kemal},
doi = {10.1016/j.compmedimag.2010.07.007},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {CAD,Density-based clustering,Dermoscopy,Image understanding},
month = mar,
number = {2},
pages = {128--136},
publisher = {Pergamon},
title = {Fast density-based lesion detection in dermoscopy images},
volume = {35},
year = {2011}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
eprint = {1606.06565},
month = jun,
title = {Concrete Problems in AI Safety},
year = {2016}
}
@inproceedings{inceptionv3,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
isbn = {9781467388504},
issn = {10636919},
month = dec,
pages = {2818--2826},
publisher = {IEEE Computer Society},
title = {Rethinking the Inception Architecture for Computer Vision},
volume = {2016-Decem},
year = {2016}
}
@article{Guo2017,
abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
archivePrefix = {arXiv},
arxivId = {1706.04599},
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
eprint = {1706.04599},
journal = {34th International Conference on Machine Learning, ICML 2017},
month = jun,
pages = {2130--2143},
publisher = {International Machine Learning Society (IMLS)},
title = {On Calibration of Modern Neural Networks},
volume = {3},
year = {2017}
}
@techreport{Canziani,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important met-rics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {1605.07678v4},
author = {Canziani, Alfredo and Culurciello, Eugenio and Paszke, Adam},
eprint = {1605.07678v4},
title = {AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
month = jul,
title = {Improving neural networks by preventing co-adaptation of feature detectors},
year = {2012}
}
@techreport{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition , computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467v2},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
eprint = {1603.04467v2},
title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}
}
@inproceedings{resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
month = dec,
pages = {770--778},
publisher = {IEEE Computer Society},
title = {Deep residual learning for image recognition},
volume = {2016-Decem},
year = {2016}
}
@book{Grus,
abstract = {Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist.},
address = {Beijing},
author = {Grus, Joel},
isbn = {978-1-4919-0142-7},
keywords = {01624 103 safari book ai software development data},
publisher = {O'Reilly},
title = {Data Science from Scratch: First Principles with Python},
url = {http://my.safaribooksonline.com/97814919-01427},
year = {2015}
}
@misc{standfordcnn,
title = {CS231n Convolutional Neural Networks for Visual Recognition},
url = {https://cs231n.github.io/convolutional-networks/},
urldate = {2020-03-15}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {Gradient-based learning applied to document recognition},
volume = {86},
year = {1998}
}
@article{Maier2015,
abstract = {Background Lately, various smartphone applications have been introduced as diagnostic self-monitoring tools in the evaluation of pigmented moles, but most of these techniques have not been evaluated systematically. Objectives The purpose of this study was to evaluate prospectively the sensitivity and specificity of a recently developed smartphone application using fractal image analysis for the risk evaluation algorithm in the diagnosis of malignant melanoma compared to clinical diagnosis and histopathological result. Methods Consecutive patients with melanocytic lesions were recruited and clinical and dermoscopical diagnosis was documented by two dermatologists independently. Imaging and analysis with the smartphone application was performed prior to excision of lesions. The findings were compared to the histological results as gold standard. Results Of 195 included lesions histopathological analysis revealed 40 melanomas, 42 dysplastic nevi and 113 benign nevi. The sensitivity of the diagnosis melanoma by fractal image analysis using smartphone images was 73{\%}, the specificity was 83{\%} compared to a sensitivity of 88{\%} and specificity of 97{\%} regarding the clinical diagnosis by the dermatologists. Conclusion The smartphone application using fractal analysis might be a promising tool in the pre-evaluation of pigmented moles by laypersons, while it is to date inferior to the diagnostic evaluation by a dermatologist.},
author = {Maier, T. and Kulichova, D. and Schotten, K. and Astrid, R. and Ruzicka, T. and Berking, C. and Udrea, A.},
doi = {10.1111/jdv.12648},
issn = {14683083},
journal = {Journal of the European Academy of Dermatology and Venereology},
month = apr,
number = {4},
pages = {663--667},
publisher = {Blackwell Publishing Ltd},
title = {Accuracy of a smartphone application using fractal image analysis of pigmented moles compared to clinical diagnosis and histological result},
volume = {29},
year = {2015}
}
@article{Izmailov2018,
abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
archivePrefix = {arXiv},
arxivId = {1803.05407},
author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
eprint = {1803.05407},
journal = {34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018},
month = mar,
pages = {876--885},
publisher = {Association For Uncertainty in Artificial Intelligence (AUAI)},
title = {Averaging Weights Leads to Wider Optima and Better Generalization},
volume = {2},
year = {2018}
}
@article{Henning2007,
abstract = {Background: The color, architecture, symmetry, and homogeneity (CASH) algorithm for dermoscopy includes a feature not used in prior algorithms, namely, architecture. Architectural order/disorder is derived from current concepts regarding the biology of benign versus malignant melanocytic neoplasms. Objective: We sought to evaluate the accuracy of the CASH algorithm. Methods: A total CASH score (TCS) was calculated for dermoscopic images of 325 melanocytic neoplasms. Sensitivity, specificity, diagnostic accuracy, and receiver operating characteristic curve analyses were performed by comparing the TCS with the histopathologic diagnoses for all lesions. Results: The mean TCS was 12.28 for melanoma, 7.62 for dysplastic nevi, and 5.24 for nondysplastic nevi. These differences were statistically significant (P {\textless} .001). A TCS of 8 or more yielded a sensitivity of 98{\%} and specificity of 68{\%} for the diagnosis of melanoma. Limitations: This is a single-evaluator pilot study. Additional studies are needed to verify the CASH algorithm. Conclusions: The CASH algorithm can distinguish melanoma from melanocytic nevi with sensitivity and specificity comparable with other algorithms. Further study is warranted to determine its intraobserver and interobserver correlations. {\textcopyright} 2007 American Academy of Dermatology, Inc.},
author = {Henning, J. Scott and Dusza, Stephen W. and Wang, Steven Q. and Marghoob, Ashfaq A. and Rabinovitz, Harold S. and Polsky, David and Kopf, Alfred W.},
doi = {10.1016/j.jaad.2006.09.003},
issn = {01909622},
journal = {Journal of the American Academy of Dermatology},
month = jan,
number = {1},
pages = {45--52},
publisher = {J Am Acad Dermatol},
title = {The CASH (color, architecture, symmetry, and homogeneity) algorithm for dermoscopy},
volume = {56},
year = {2007}
}
@article{Jaworek-Korjakowska2018,
abstract = {Background. Malignant melanoma is among the fastest increasing malignancies in many countries. With the help of new tools, such as teledermoscopy referrals between primary healthcare and dermatology clinics, the diagnosis of these patients could be made more efficient. The introduction of a high-quality smartphone with a built-in digital camera may make the early detection more convenient. This study presents novel directions for early detection of malignant melanoma based on a smartphone application. Objectives and Methods. In this study, we concentrate on a precise description of a complex infrastructure of a fully automated computer-aided diagnostic system for early detection of malignant melanoma. The framework has been customized for a dermoscope that is customized to attach to the smartphone to be able to carry out mobile teledermoscopy. The application requirements, architecture, and computational methods as well as behavioral and dynamic aspects have been presented in this paper. Conclusion. This paper presents a broad application architecture, which can be easily customized for rapid deployment of a sophisticated health application. Mobile teledermoscopy is a new horizon that might become in the future the basis of the early detection of pigmented skin lesions as a screening tool for primary care doctors and inexperienced dermatologists.},
author = {Jaworek-Korjakowska, Joanna and Kleczek, Pawel},
doi = {10.1155/2018/5767360},
issn = {15308677},
journal = {Wireless Communications and Mobile Computing},
publisher = {Hindawi Limited},
title = {ESkin: Study on the smartphone application for early detection of malignant melanoma},
volume = {2018},
year = {2018}
}
@inproceedings{inceptionv1,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
month = oct,
pages = {1--9},
publisher = {IEEE Computer Society},
title = {Going deeper with convolutions},
volume = {07-12-June},
year = {2015}
}
@misc{Ly2019,
abstract = {Deep learning neural networks have made significant progress in image analysis and have been used for skin cancer recognition. Early detection and proper treatments for malignant skin cancer cases are vital to ensure high survival rate in patients. We present a novel deep learning based convolutional neural network (CNN) model for generating compatible models on mobile platforms such as Android and iOS. The proposed model was tested on the grand challenge PHDB melanoma dataset. The best performing proposed model excels in the following ways: (1) it outperforms the baseline model in terms of accuracy by 1{\%}; (2) it consists of 60{\%} fewer parameters compared to the base model and thereby it is more efficient on mobile platforms. Furthermore, the model is more compact and retains high accuracy without the need to be downsized; (3) in conjunction with advanced regularization techniques such as dropout and data augmentation, the proposed CNN model excelled when implemented on state-of-the-art frameworks such as Keras and TensorFlow. Additionally, we were able to successfully deploy it on the iOS and Android mobile systems. The proposed model could also be lucrative towards other datasets for image classification on mobile platform},
author = {Ly, Phillip and Bein, Doina and Verma, Abhishek},
doi = {10.1109/uemcon.2018.8796628},
isbn = {9781538676936},
keywords = {CNN,Deep Learning,Melanoma,Mobile Systems,PHDB,Skin Cancer},
month = aug,
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {New Compact Deep Learning Model for Skin Cancer Recognition},
year = {2019}
}
@article{Milton2018,
abstract = {In this paper, we studied extensively on different deep learning based methods to detect melanoma and skin lesion cancers. Melanoma, a form of malignant skin cancer is very threatening to health. Proper diagnosis of melanoma at an earlier stage is crucial for the success rate of complete cure. Dermoscopic images with Benign and malignant forms of skin cancer can be analyzed by computer vision system to streamline the process of skin cancer detection. In this study, we experimented with various neural networks which employ recent deep learning based models like PNASNet-5-Large, InceptionResNetV2, SENet154, InceptionV4. Dermoscopic images are properly processed and augmented before feeding them into the network. We tested our methods on International Skin Imaging Collaboration (ISIC) 2018 challenge dataset. Our system has achieved best validation score of 0.76 for PNASNet-5-Large model. Further improvement and optimization of the proposed methods with a bigger training dataset and carefully chosen hyper-parameter could improve the performances. The code available for download at https://github.com/miltonbd/ISIC{\_}2018{\_}classification},
archivePrefix = {arXiv},
arxivId = {1901.10802},
author = {Milton, Md Ashraful Alam},
eprint = {1901.10802},
month = jan,
title = {Automated Skin Lesion Classification Using Ensemble of Deep Neural Networks in ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection Challenge},
year = {2019}
}
@misc{isic2020,
doi = {https://doi.org/10.34970/2020-ds01},
publisher = {ISDIS},
title = {The International Skin Imaging Collaboration (ISIC) 2020 Challenge Dataset},
urldate = {2020-06-09},
year = {2020}
}
@article{Tajbakhsh2016,
abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
archivePrefix = {arXiv},
arxivId = {1706.00712},
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
doi = {10.1109/TMI.2016.2535302},
eprint = {1706.00712},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Carotid intima-media thickness,computer-aided detection,convolutional neural networks,deep learning,fine-tuning,medical image analysis,polyp detection,pulmonary embolism detection,video quality assessment},
month = may,
number = {5},
pages = {1299--1312},
pmid = {26978662},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?},
volume = {35},
year = {2016}
}
@misc{marctransferlearning,
author = {Marcelino, Pedro},
title = {Transfer learning from pre-trained models - Towards Data Science},
url = {https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751},
urldate = {2019-12-02},
year = {2018}
}
@misc{Ali2017,
abstract = {In this paper, we propose a method for classifying melanoma images into benign and malignant using Convolutional Neural Networks (CNNs). Having an automated method for melanoma detection will assist dermatologists in the early diagnosis of this type of skin cancer. A regular convolutional network employing a modest number of parameters is used to detect melanoma images. The architecture is used to classify the dataset of the ISBI 2016 challenge in melanoma classification. The dataset was not segmented or cropped prior to classification. The proposed method was then evaluated for accuracy, sensitivity and specificity. Comparisons with the winning entry in the competition demonstrate that one can achieve a performance level comparable to state-of-the-art using standard convolutional neural network architectures that employ a lower number of parameters.},
author = {Ali, Aya Abu and Al-Marzouqi, Hasan},
booktitle = {2017 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2017},
doi = {10.1109/ICECTA.2017.8252041},
isbn = {9781538608722},
month = jun,
pages = {1--5},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Melanoma detection using regular convolutional neural networks},
volume = {2018-Janua},
year = {2017}
}
@inproceedings{Hendrycks2019,
abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
archivePrefix = {arXiv},
arxivId = {1610.02136},
author = {Hendrycks, Dan and Gimpel, Kevin},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1610.02136},
month = oct,
publisher = {ICLR},
title = {A baseline for detecting misclassified and out-of-distribution examples in neural networks},
year = {2016}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
journal = {Advances in Neural Information Processing Systems},
month = nov,
number = {January},
pages = {3320--3328},
publisher = {Neural information processing systems foundation},
title = {How transferable are features in deep neural networks?},
volume = {4},
year = {2014}
}
@misc{yu,
abstract = {Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, respectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.},
author = {Yu, Lequan and Chen, Hao and Dou, Qi and Qin, Jing and Heng, Pheng Ann},
booktitle = {IEEE Transactions on Medical Imaging},
doi = {10.1109/TMI.2016.2642839},
issn = {1558254X},
keywords = {Automated melanoma recognition,fully convolutional neural networks,residual learning,skin lesion analysis,very deep convolutional neural networks},
month = apr,
number = {4},
pages = {994--1004},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks},
volume = {36},
year = {2017}
}
@article{odin,
abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7{\%} to 4.3{\%} on the DenseNet (applied to CIFAR-10) when the true positive rate is 95{\%}.},
archivePrefix = {arXiv},
arxivId = {1706.02690},
author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
eprint = {1706.02690},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = jun,
publisher = {ICLR},
title = {Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
year = {2017}
}
@inproceedings{Deng2010,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
doi = {10.1109/cvpr.2009.5206848},
month = mar,
pages = {248--255},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {ImageNet: A large-scale hierarchical image database},
year = {2010}
}
@misc{Argenziano2001,
abstract = {The clinical use of dermoscopy has uncovered a new and fascinating morphological dimension of pigmented skin lesions. Dermoscopy is a non-invasive diagnostic technique that links clinical dermatology and dermatopathology by enabling the visualisation of morphological features not seen by the naked eye. Close examination of pigmented skin lesions in this way increases the effectiveness of clinical diagnostic tools by providing new morphological criteria for distinguishing melanoma from other melanocytic and non-melanocytic pigmented skin lesions.In the past, dermoscopy has been known by various names, including skin surface microscopy, epiluminescence microscopy, incident light microscopy, dermatoscopy, and videodermatoscopy. However, the term 'dermoscopy', first used by Friedman and colleagues in 1991 1, is the most widely used. {\textcopyright} 2001 Elsevier Science Ltd.},
author = {Argenziano, Giuseppe and Soyer, H. Peter},
booktitle = {Lancet Oncology},
doi = {10.1016/S1470-2045(00)00422-8},
issn = {14702045},
month = jul,
number = {7},
pages = {443--449},
pmid = {11905739},
publisher = {Lancet Publishing Group},
title = {Dermoscopy of pigmented skin lesions - a valuable tool for early diagnosis of melanoma},
volume = {2},
year = {2001}
}
@article{Yu2015,
abstract = {While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.},
archivePrefix = {arXiv},
arxivId = {1506.03365},
author = {Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
eprint = {1506.03365},
month = jun,
title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
year = {2015}
}
@misc{Brownlee,
author = {Brownlee, Jason},
title = {Ensemble Learning Methods for Deep Learning Neural Networks},
url = {https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/},
urldate = {2020-03-16},
year = {2018}
}
@article{Cybenko1989,
abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks. {\textcopyright} 1989 Springer-Verlag New York Inc.},
author = {Cybenko, G.},
doi = {10.1007/BF02551274},
issn = {09324194},
journal = {Mathematics of Control, Signals, and Systems},
keywords = {Approximation,Completeness,Neural networks},
month = dec,
number = {4},
pages = {303--314},
publisher = {Springer-Verlag},
title = {Approximation by superpositions of a sigmoidal function},
volume = {2},
year = {1989}
}
@article{Bray2018,
abstract = {This article provides a status report on the global burden of cancer worldwide using the GLOBOCAN 2018 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer, with a focus on geographic variability across 20 world regions. There will be an estimated 18.1 million new cancer cases (17.0 million excluding nonmelanoma skin cancer) and 9.6 million cancer deaths (9.5 million excluding nonmelanoma skin cancer) in 2018. In both sexes combined, lung cancer is the most commonly diagnosed cancer (11.6{\%} of the total cases) and the leading cause of cancer death (18.4{\%} of the total cancer deaths), closely followed by female breast cancer (11.6{\%}), prostate cancer (7.1{\%}), and colorectal cancer (6.1{\%}) for incidence and colorectal cancer (9.2{\%}), stomach cancer (8.2{\%}), and liver cancer (8.2{\%}) for mortality. Lung cancer is the most frequent cancer and the leading cause of cancer death among males, followed by prostate and colorectal cancer (for incidence) and liver and stomach cancer (for mortality). Among females, breast cancer is the most commonly diagnosed cancer and the leading cause of cancer death, followed by colorectal and lung cancer (for incidence), and vice versa (for mortality); cervical cancer ranks fourth for both incidence and mortality. The most frequently diagnosed cancer and the leading cause of cancer death, however, substantially vary across countries and within each country depending on the degree of economic development and associated social and life style factors. It is noteworthy that high-quality cancer registry data, the basis for planning and implementing evidence-based cancer control programs, are not available in most low- and middle-income countries. The Global Initiative for Cancer Registry Development is an international partnership that supports better estimation, as well as the collection and use of local data, to prioritize and evaluate national cancer control efforts. CA: A Cancer Journal for Clinicians 2018;0:1-31. {\textcopyright} 2018 American Cancer Society.},
author = {Bray, Freddie and Ferlay, Jacques and Soerjomataram, Isabelle and Siegel, Rebecca L. and Torre, Lindsey A. and Jemal, Ahmedin},
doi = {10.3322/caac.21492},
issn = {1542-4863},
journal = {CA: A Cancer Journal for Clinicians},
month = nov,
number = {6},
pages = {394--424},
pmid = {30207593},
publisher = {American Cancer Society},
title = {Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries},
volume = {68},
year = {2018}
}
@misc{dermengine,
title = {DermEngine | Visual Search},
url = {https://www.dermengine.com/en-ca/visual-search},
urldate = {2019-11-21}
}
@article{Lucieri2020,
abstract = {Deep learning based medical image classifiers have shown remarkable prowess in various application areas like ophthalmology, dermatology, pathology, and radiology. However, the acceptance of these Computer-Aided Diagnosis (CAD) systems in real clinical setups is severely limited primarily because their decision-making process remains largely obscure. This work aims at elucidating a deep learning based medical image classifier by verifying that the model learns and utilizes similar disease-related concepts as described and employed by dermatologists. We used a well-trained and high performing neural network developed by REasoning for COmplex Data (RECOD) Lab for classification of three skin tumours, i.e. Melanocytic Naevi, Melanoma and Seborrheic Keratosis and performed a detailed analysis on its latent space. Two well established and publicly available skin disease datasets, PH2 and derm7pt, are used for experimentation. Human understandable concepts are mapped to RECOD image classification model with the help of Concept Activation Vectors (CAVs), introducing a novel training and significance testing paradigm for CAVs. Our results on an independent evaluation set clearly shows that the classifier learns and encodes human understandable concepts in its latent representation. Additionally, TCAV scores (Testing with CAVs) suggest that the neural network indeed makes use of disease-related concepts in the correct way when making predictions. We anticipate that this work can not only increase confidence of medical practitioners on CAD but also serve as a stepping stone for further development of CAV-based neural network interpretation methods.},
archivePrefix = {arXiv},
arxivId = {2005.02000},
author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Braun, Stephan Alexander and Malik, Muhammad Imran and Dengel, Andreas and Ahmed, Sheraz},
eprint = {2005.02000},
month = may,
title = {On Interpretability of Deep Learning based Skin Lesion Classifiers using Concept Activation Vectors},
year = {2020}
}
@techreport{isic2019second,
abstract = {On the other hand, the images for skin lesion analyzing may be taken by different devices, producing huge variations in scale, resolution, lighting condition, rotation angles etc. Besides, the number of samples for different lesion category can be heavily imbalanced, due to the difference in morbidity over populations. For classification problem, a predictive model would have better performance if the input are sampled from the same distribution. Thus, a carefully designed Abstract: Deep learning based algorithm have become been the first option for analyzing medical images. For skin lesion classification task, dermoscopy imaging is an effective approach of generating high resolution images. The scale and resolution of skin lesion can be of huge variations in the captured images. On the other hand, the number of labelled skin lesion images is relative small for training deep models with well performance. In this article, we describes the motivation, design and results of our approaches for the multi-class skin lesion classification problem, using the dermoscopy images provided in ISIC 2019 Challenge. Our best method can achieve a average accuracy of 75.3{\%} over all the 8 categories based on 5-fold cross validation.},
author = {Zhou, Steven and Zhuang, Yixin and Meng, Rusong},
institution = {DysionAI},
title = {Multi-Category Skin Lesion Diagnosis Using Dermoscopy Images and Deep CNN Ensembles},
year = {2019}
}
@article{Wong2011,
abstract = {An automatic method for segmenting skin lesions in conventional macroscopic images is presented. The images are acquired with conventional cameras, without the use of a dermoscope. Automatic segmentation of skin lesions from macroscopic images is a very challenging problem due to factors such as illumination variations, irregular structural and color variations, the presence of hair, as well as the occurrence of multiple unhealthy skin regions. To address these factors, a novel iterative stochastic region-merging approach is employed to segment the regions corresponding to skin lesions from the macroscopic images, where stochastic region merging is initialized first on a pixel level, and subsequently on a region level until convergence. A region merging likelihood function based on the regional statistics is introduced to determine the merger of regions in a stochastic manner. Experimental results show that the proposed system achieves overall segmentation error of under 10 for skin lesions in macroscopic images, which is lower than that achieved by existing methods. {\textcopyright} 2011 IEEE.},
author = {Wong, Alexander and Scharcanski, Jacob and Fieguth, Paul},
doi = {10.1109/TITB.2011.2157829},
issn = {10897771},
journal = {IEEE Transactions on Information Technology in Biomedicine},
keywords = {Iterative,lesion,region merging,skin cancer,stochastic},
month = nov,
number = {6},
pages = {929--936},
title = {Automatic skin lesion segmentation via iterative stochastic region merging},
volume = {15},
year = {2011}
}
@techreport{Krizhevskya,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels.},
author = {Krizhevsky, Alex},
month = apr,
title = {Learning Multiple Layers of Features from Tiny Images},
url = {http://www.cs.toronto.edu/{~}kriz/cifar.html},
year = {2009}
}
@book{Goodfellow-et-al-2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {Deep Learning},
url = {http://www.deeplearningbook.org},
year = {2016}
}
