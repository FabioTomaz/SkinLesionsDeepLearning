Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{sgdr,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1608.03983},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loshchilov, Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restarts.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {aug},
publisher = {International Conference on Learning Representations, ICLR},
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
url = {http://arxiv.org/abs/1608.03983},
year = {2016}
}
@article{cutout,
abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56{\%}, 15.20{\%}, and 1.30{\%} test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
archivePrefix = {arXiv},
arxivId = {1708.04552},
author = {DeVries, Terrance and Taylor, Graham W.},
eprint = {1708.04552},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeVries, Taylor - 2017 - Improved Regularization of Convolutional Neural Networks with Cutout.pdf:pdf},
month = {aug},
title = {{Improved Regularization of Convolutional Neural Networks with Cutout}},
url = {http://arxiv.org/abs/1708.04552},
year = {2017}
}
@book{Grus,
abstract = {Book about data science},
author = {Grus, Joel},
file = {:home/fabio/mestrado/dissertacao/books/[Joel{\_}Grus]{\_}Data{\_}Science{\_}from{\_}Scratch{\_}First{\_}Princ.pdf:pdf},
isbn = {9781491901427},
publisher = {O'Reilly Media},
title = {{Data Science From Scratch}},
year = {2015}
}
@article{Jaworek-Korjakowska2018,
abstract = {Background. Malignant melanoma is among the fastest increasing malignancies in many countries. With the help of new tools, such as teledermoscopy referrals between primary healthcare and dermatology clinics, the diagnosis of these patients could be made more efficient. The introduction of a high-quality smartphone with a built-in digital camera may make the early detection more convenient. This study presents novel directions for early detection of malignant melanoma based on a smartphone application. Objectives and Methods. In this study, we concentrate on a precise description of a complex infrastructure of a fully automated computer-aided diagnostic system for early detection of malignant melanoma. The framework has been customized for a dermoscope that is customized to attach to the smartphone to be able to carry out mobile teledermoscopy. The application requirements, architecture, and computational methods as well as behavioral and dynamic aspects have been presented in this paper. Conclusion. This paper presents a broad application architecture, which can be easily customized for rapid deployment of a sophisticated health application. Mobile teledermoscopy is a new horizon that might become in the future the basis of the early detection of pigmented skin lesions as a screening tool for primary care doctors and inexperienced dermatologists.},
author = {Jaworek-Korjakowska, Joanna and Kleczek, Pawel},
doi = {10.1155/2018/5767360},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaworek-Korjakowska, Kleczek - 2018 - ESkin Study on the smartphone application for early detection of malignant melanoma.pdf:pdf},
issn = {15308677},
journal = {Wireless Communications and Mobile Computing},
publisher = {Hindawi Limited},
title = {{ESkin: Study on the smartphone application for early detection of malignant melanoma}},
volume = {2018},
year = {2018}
}
@inproceedings{Pham2018,
abstract = {Deep CNN techniques have dramatically become the state of the art in image classification. However, applying high-capacity Deep CNN in medical image analysis has been impeded because of scarcity of labeled data. This study has two primary contributions: first, we propose a classification model to improve performance of classification of skin lesion using Deep CNN and Data Augmentation. Second, we demonstrate the use of image data augmentation for overcoming the problem of data limitation and examine the influence of different number of augmented samples on the performance of different classifiers. The proposed classification system is evaluated using the largest public skin lesion testing dataset, containing 600 testing images, and 6,162 training images. New state-of-the-art performance result is archived with AUC (89.2{\%} vs. 87.4{\%}), AP (73.9{\%} vs. 71.5{\%}), and ACC (89.0{\%} vs. 87.2{\%}). In additional, we explore the influence of each image augmentation on the three classifiers and observe that performance of each classifier is influenced differently by each augmentation and has better results comparing with traditional methods. Thus, it is suggested that the performance of skin cancer classification and medial image classification could be improved further by applying data augmentation.},
author = {Pham, Tri Cong and Luong, Chi Mai and Visani, Muriel and Hoang, Van Dung},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-75420-8_54},
file = {:home/fabio/Desktop/OfficialPubli{\_}10752{\_}ACIIDS18{\_}DeepCNNandDataAugmentationforSkinLesionClassification.pdf:pdf},
isbn = {9783319754192},
issn = {16113349},
keywords = {Data augmentation,Deep learning,Medical image,Melanoma classification,Skin cancer},
pages = {573--582},
publisher = {Springer Verlag},
title = {{Deep CNN and Data Augmentation for Skin Lesion Classification}},
volume = {10752 LNAI},
year = {2018}
}
@misc{isic2019,
title = {{ISIC 2019}},
url = {https://challenge2019.isic-archive.com/},
urldate = {2019-11-23},
year = {2019}
}
@techreport{Canziani,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important met-rics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {1605.07678v4},
author = {Canziani, Alfredo and Culurciello, Eugenio and Paszke, Adam},
eprint = {1605.07678v4},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canziani, Culurciello, Paszke - Unknown - AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS.pdf:pdf},
title = {{AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS}}
}
@article{Hennemann2017,
abstract = {eHealth interventions can be effective in treating health problems. However, adoption in inpatient routine care seems limited. The present study therefore aimed to investigate barriers and facilitators to acceptance of eHealth interventions and of online aftercare in particular in health professionals of inpatient treatment. A total of 152 out of 287 health professionals of various professional groups in four inpatient rehabilitation facilities filled out a self-administered web-based questionnaire (response rate: 53{\%}); 128 individuals were eligible for further data analysis. Acceptance and possible predictors were investigated with a complex research model based on the Unified Theory of Acceptance and Use of Technology. Acceptance of eHealth interventions was rather low (M = 2.47, SD = 0.98); however, acceptance of online aftercare was moderate (M = 3.08, SD = 0.96, t(127) = 8.22, p {\textless}.001), and eHealth literacy was elevated. Social influence, performance expectancy, and treatment-related internet and mobile use significantly predicted overall acceptance. No differences were found between professional and age groups. Although acceptance of eHealth interventions was limited in health professionals of inpatient treatment, moderate acceptance of online aftercare for work-related stress implies a basis for future implementation. Tailored eHealth education addressing misconceptions about inferiority and incongruity with conventional treatment considering the systemic aspect of acceptance formation are needed.},
author = {Hennemann, Severin and Beutel, Manfred E. and Zwerenz, R{\"{u}}diger},
doi = {10.1080/10810730.2017.1284286},
issn = {10870415},
journal = {Journal of Health Communication},
month = {mar},
number = {3},
pages = {274--284},
pmid = {28248626},
publisher = {Taylor and Francis Inc.},
title = {{Ready for eHealth? Health Professionals' Acceptance and Adoption of eHealth Interventions in Inpatient Routine Care}},
volume = {22},
year = {2017}
}
@article{Milton2018,
abstract = {In this paper, we studied extensively on different deep learning based methods to detect melanoma and skin lesion cancers. Melanoma, a form of malignant skin cancer is very threatening to health. Proper diagnosis of melanoma at an earlier stage is crucial for the success rate of complete cure. Dermoscopic images with Benign and malignant forms of skin cancer can be analyzed by computer vision system to streamline the process of skin cancer detection. In this study, we experimented with various neural networks which employ recent deep learning based models like PNASNet-5-Large, InceptionResNetV2, SENet154, InceptionV4. Dermoscopic images are properly processed and augmented before feeding them into the network. We tested our methods on International Skin Imaging Collaboration (ISIC) 2018 challenge dataset. Our system has achieved best validation score of 0.76 for PNASNet-5-Large model. Further improvement and optimization of the proposed methods with a bigger training dataset and carefully chosen hyper-parameter could improve the performances. The code available for download at https://github.com/miltonbd/ISIC{\_}2018{\_}classification},
archivePrefix = {arXiv},
arxivId = {1901.10802},
author = {Milton, Md Ashraful Alam},
eprint = {1901.10802},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milton - 2019 - Automated Skin Lesion Classification Using Ensemble of Deep Neural Networks in ISIC 2018 Skin Lesion Analysis Towards Me.pdf:pdf},
month = {jan},
title = {{Automated Skin Lesion Classification Using Ensemble of Deep Neural Networks in ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection Challenge}},
url = {http://arxiv.org/abs/1901.10802},
year = {2019}
}
@article{densenet,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
eprint = {1608.06993},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
month = {aug},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@article{Yuan2017,
abstract = {Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge},
archivePrefix = {arXiv},
arxivId = {1709.09780},
author = {Yuan, Yading and Lo, Yeh-Chi},
eprint = {1709.09780},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Lo - 2017 - Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks(2).pdf:pdf},
month = {sep},
title = {{Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks}},
url = {http://arxiv.org/abs/1709.09780},
year = {2017}
}
@article{Bissoto2018,
abstract = {This extended abstract describes the participation of RECOD Titans in parts 1 to 3 of the ISIC Challenge 2018 "Skin Lesion Analysis Towards Melanoma Detection" (MICCAI 2018). Although our team has a long experience with melanoma classification and moderate experience with lesion segmentation, the ISIC Challenge 2018 was the very first time we worked on lesion attribute detection. For each task we submitted 3 different ensemble approaches, varying combinations of models and datasets. Our best results on the official testing set, regarding the official metric of each task, were: 0.728 (segmentation), 0.344 (attribute detection) and 0.803 (classification). Those submissions reached, respectively, the 56th, 14th and 9th places.},
archivePrefix = {arXiv},
arxivId = {1808.08480},
author = {Bissoto, Alceu and Perez, F{\'{a}}bio and Ribeiro, Vin{\'{i}}cius and Fornaciali, Michel and Avila, Sandra and Valle, Eduardo},
eprint = {1808.08480},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bissoto et al. - 2018 - Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis, Classification RECOD Titans at ISIC Challenge 20.pdf:pdf},
month = {aug},
title = {{Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis, Classification: RECOD Titans at ISIC Challenge 2018}},
url = {http://arxiv.org/abs/1808.08480},
year = {2018}
}
@article{alexnet,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
month = {jun},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {{ImageNet classification with deep convolutional neural networks}},
volume = {60},
year = {2012}
}
@misc{standfordcnn,
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/convolutional-networks/},
urldate = {2020-03-15}
}
@techreport{isic2019third,
author = {Pollastri, Federico and Maro{\~{n}}as, Juan and Parre{\~{n}}o, Mario and Bolelli, Federico and Paredes, Roberto and Grana, Costantino and Albiol, Alberto},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollastri et al. - Unknown - AImageLab-PRHLT at ISIC Challenge 2019.pdf:pdf},
title = {{AImageLab-PRHLT at ISIC Challenge 2019}},
year = {2019}
}
@inproceedings{inceptionv4,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
eprint = {1602.07261},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2017 - Inception-v4, inception-ResNet and the impact of residual connections on learning.pdf:pdf},
month = {feb},
pages = {4278--4284},
publisher = {AAAI press},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@phdthesis{marques2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Marques, Jo{\~{a}}o},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/fabio/mestrado/dissertacao/related{\_}thesis/Joao{\_}Marques/JoaoMarques{\_}Dissertation{\_}SelfSurveillanceSystemChangeDetection.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
school = {University of Aveiro},
title = {{A Self-Surveillance System for Change Detection of Pigmented Skin Lesions}},
year = {2019}
}
@book{Goodfellow-et-al-2016,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@inproceedings{Deng2010,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
doi = {10.1109/cvpr.2009.5206848},
month = {mar},
pages = {248--255},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2010}
}
@inproceedings{Pomponiu2016,
abstract = {Nowadays, the occurrence of skin cancer cases has grown worldwide due to the extended exposure to the harmful radiation from the Sun. Most common approach to detect the malignancy of skin moles is by visual inspection performed by an expert dermatologist, using a set of specific clinical rules. Computer-aided diagnosis, based on skin mole imaging, is another concurrent method which has experienced major advancements due to improvement of imaging sensors and processing power. However, these schemes use hand-crafted features which are difficult to tune and perform poorly on new cases due to lack of generalization power. In this study we present a method that use a pretrained deep neural network (DNN) to automatically extract a set of representative features that can be later used to diagnose a sample of skin lesion for malignancy. The experimental tests carried out on a clinical dataset show that the classification performance using DNN-based features performs better than the state-of-the-art techniques.},
author = {Pomponiu, V. and Nejati, H. and Cheung, N. M.},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7532834},
isbn = {9781467399616},
issn = {15224880},
keywords = {Deep neural networks,Feature extraction,Malignant melanoma,Skin mole classification,Transfer learning},
month = {aug},
pages = {2623--2627},
publisher = {IEEE Computer Society},
title = {{Deepmole: Deep neural networks for skin mole lesion classification}},
volume = {2016-Augus},
year = {2016}
}
@misc{isic2018top3,
author = {Nozdryn-Plotnicki, Aleksey and Yap, Jordan and Yolland, William},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nozdryn-Plotnicki, Yap, Yolland - Unknown - Ensembling Convolutional Neural Networks for Skin Cancer Classification.pdf:pdf},
title = {{Ensembling Convolutional Neural Networks for Skin Cancer Classification}},
year = {2018}
}
@article{Barata2014,
abstract = {Melanoma is one of the deadliest forms of cancer; hence, great effort has been put into the development of diagnosis methods for this disease. This paper addresses two different systems for the detection of melanomas in dermoscopy images. The first system uses global methods to classify skin lesions, whereas the second system uses local features and the bag-of-features classifier. This paper aims at determining the best system for skin lesion classification. The other objective is to compare the role of color and texture features in lesion classification and determine which set of features is more discriminative. It is concluded that color features outperform texture features when used alone and that both methods achieve very good results, i.e., Sensitivity = 96{\%} and Specificity = 80{\%} for global methods against Sensitivity = 100{\%} and Specificity = 75{\%} for local methods. The classification results were obtained on a data set of 176 dermoscopy images from Hospital Pedro Hispano, Matosinhos. {\textcopyright} 2014 IEEE.},
author = {Barata, Catarina and Ruela, Margarida and Francisco, Mariana and Mendonca, Teresa and Marques, Jorge S.},
doi = {10.1109/JSYST.2013.2271540},
file = {:home/fabio/Downloads/barata.pdf:pdf},
issn = {19379234},
journal = {IEEE Systems Journal},
keywords = {Bag of features (BoF),color,computer-aided diagnosis,dermoscopy,melanoma,texture},
number = {3},
pages = {965--979},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Two systems for the detection of melanomas in dermoscopy images using texture and color features}},
volume = {8},
year = {2014}
}
@article{Bray2018,
abstract = {This article provides a status report on the global burden of cancer worldwide using the GLOBOCAN 2018 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer, with a focus on geographic variability across 20 world regions. There will be an estimated 18.1 million new cancer cases (17.0 million excluding nonmelanoma skin cancer) and 9.6 million cancer deaths (9.5 million excluding nonmelanoma skin cancer) in 2018. In both sexes combined, lung cancer is the most commonly diagnosed cancer (11.6{\%} of the total cases) and the leading cause of cancer death (18.4{\%} of the total cancer deaths), closely followed by female breast cancer (11.6{\%}), prostate cancer (7.1{\%}), and colorectal cancer (6.1{\%}) for incidence and colorectal cancer (9.2{\%}), stomach cancer (8.2{\%}), and liver cancer (8.2{\%}) for mortality. Lung cancer is the most frequent cancer and the leading cause of cancer death among males, followed by prostate and colorectal cancer (for incidence) and liver and stomach cancer (for mortality). Among females, breast cancer is the most commonly diagnosed cancer and the leading cause of cancer death, followed by colorectal and lung cancer (for incidence), and vice versa (for mortality); cervical cancer ranks fourth for both incidence and mortality. The most frequently diagnosed cancer and the leading cause of cancer death, however, substantially vary across countries and within each country depending on the degree of economic development and associated social and life style factors. It is noteworthy that high-quality cancer registry data, the basis for planning and implementing evidence-based cancer control programs, are not available in most low- and middle-income countries. The Global Initiative for Cancer Registry Development is an international partnership that supports better estimation, as well as the collection and use of local data, to prioritize and evaluate national cancer control efforts. CA: A Cancer Journal for Clinicians 2018;0:1-31. {\textcopyright} 2018 American Cancer Society.},
author = {Bray, Freddie and Ferlay, Jacques and Soerjomataram, Isabelle and Siegel, Rebecca L. and Torre, Lindsey A. and Jemal, Ahmedin},
doi = {10.3322/caac.21492},
issn = {1542-4863},
journal = {CA: A Cancer Journal for Clinicians},
month = {nov},
number = {6},
pages = {394--424},
pmid = {30207593},
publisher = {American Cancer Society},
title = {{Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries}},
volume = {68},
year = {2018}
}
@inproceedings{inceptionv3,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2{\%} top-1 and 5:6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5{\%} top-5 error and 17:3{\%} top-1 error on the validation set and 3:6{\%} top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {2818--2826},
publisher = {IEEE Computer Society},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}
@techreport{pytorch,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paszke et al. - Unknown - Automatic differentiation in PyTorch.pdf:pdf},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{resnetv2,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1603.05027},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
month = {mar},
pages = {630--645},
publisher = {Springer Verlag},
title = {{Identity Mappings in Deep Residual Networks}},
url = {http://arxiv.org/abs/1603.05027},
volume = {9908 LNCS},
year = {2016}
}
@techreport{isic2019fifth,
author = {Chouhan, Vikas},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chouhan - Unknown - Skin Lesion Analysis towards Melanoma Detection with Deep Convolutional Neural Network.pdf:pdf},
title = {{Skin Lesion Analysis towards Melanoma Detection with Deep Convolutional Neural Network}},
year = {2019}
}
@article{odin,
abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7{\%} to 4.3{\%} on the DenseNet (applied to CIFAR-10) when the true positive rate is 95{\%}.},
annote = {{\'{E}} o odin usado pelo wang. Muito citado no scopus e facilmente implementavel :)},
archivePrefix = {arXiv},
arxivId = {1706.02690},
author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
eprint = {1706.02690},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Li, Srikant - 2017 - Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {jun},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}},
url = {http://arxiv.org/abs/1706.02690 https://github.com/facebookresearch/odin},
year = {2017}
}
@misc{isic2019second,
abstract = {On the other hand, the images for skin lesion analyzing may be taken by different devices, producing huge variations in scale, resolution, lighting condition, rotation angles etc. Besides, the number of samples for different lesion category can be heavily imbalanced, due to the difference in morbidity over populations. For classification problem, a predictive model would have better performance if the input are sampled from the same distribution. Thus, a carefully designed Abstract: Deep learning based algorithm have become been the first option for analyzing medical images. For skin lesion classification task, dermoscopy imaging is an effective approach of generating high resolution images. The scale and resolution of skin lesion can be of huge variations in the captured images. On the other hand, the number of labelled skin lesion images is relative small for training deep models with well performance. In this article, we describes the motivation, design and results of our approaches for the multi-class skin lesion classification problem, using the dermoscopy images provided in ISIC 2019 Challenge. Our best method can achieve a average accuracy of 75.3{\%} over all the 8 categories based on 5-fold cross validation.},
author = {Zhou, Steven and Zhuang, Yixin and Meng, Rusong},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Zhuang, Meng - Unknown - Multi-Category Skin Lesion Diagnosis Using Dermoscopy Images and Deep CNN Ensembles.pdf:pdf},
title = {{Multi-Category Skin Lesion Diagnosis Using Dermoscopy Images and Deep CNN Ensembles}},
year = {2019}
}
@article{humanvsisic2018,
abstract = {Background: Whether machine-learning algorithms can diagnose all pigmented skin lesions as accurately as human experts is unclear. The aim of this study was to compare the diagnostic accuracy of state-of-the-art machine-learning algorithms with human readers for all clinically relevant types of benign and malignant pigmented skin lesions. Methods: For this open, web-based, international, diagnostic study, human readers were asked to diagnose dermatoscopic images selected randomly in 30-image batches from a test set of 1511 images. The diagnoses from human readers were compared with those of 139 algorithms created by 77 machine-learning labs, who participated in the International Skin Imaging Collaboration 2018 challenge and received a training set of 10 015 images in advance. The ground truth of each lesion fell into one of seven predefined disease categories: intraepithelial carcinoma including actinic keratoses and Bowen's disease; basal cell carcinoma; benign keratinocytic lesions including solar lentigo, seborrheic keratosis and lichen planus-like keratosis; dermatofibroma; melanoma; melanocytic nevus; and vascular lesions. The two main outcomes were the differences in the number of correct specific diagnoses per batch between all human readers and the top three algorithms, and between human experts and the top three algorithms. Findings: Between Aug 4, 2018, and Sept 30, 2018, 511 human readers from 63 countries had at least one attempt in the reader study. 283 (55{\textperiodcentered}4{\%}) of 511 human readers were board-certified dermatologists, 118 (23{\textperiodcentered}1{\%}) were dermatology residents, and 83 (16{\textperiodcentered}2{\%}) were general practitioners. When comparing all human readers with all machine-learning algorithms, the algorithms achieved a mean of 2{\textperiodcentered}01 (95{\%} CI 1{\textperiodcentered}97 to 2{\textperiodcentered}04; p{\textless}0{\textperiodcentered}0001) more correct diagnoses (17{\textperiodcentered}91 [SD 3{\textperiodcentered}42] vs 19{\textperiodcentered}92 [4{\textperiodcentered}27]). 27 human experts with more than 10 years of experience achieved a mean of 18{\textperiodcentered}78 (SD 3{\textperiodcentered}15) correct answers, compared with 25{\textperiodcentered}43 (1{\textperiodcentered}95) correct answers for the top three machine algorithms (mean difference 6{\textperiodcentered}65, 95{\%} CI 6{\textperiodcentered}06–7{\textperiodcentered}25; p{\textless}0{\textperiodcentered}0001). The difference between human experts and the top three algorithms was significantly lower for images in the test set that were collected from sources not included in the training set (human underperformance of 11{\textperiodcentered}4{\%}, 95{\%} CI 9{\textperiodcentered}9–12{\textperiodcentered}9 vs 3{\textperiodcentered}6{\%}, 0{\textperiodcentered}8–6{\textperiodcentered}3; p{\textless}0{\textperiodcentered}0001). Interpretation: State-of-the-art machine-learning classifiers outperformed human experts in the diagnosis of pigmented skin lesions and should have a more important role in clinical practice. However, a possible limitation of these algorithms is their decreased performance for out-of-distribution images, which should be addressed in future research. Funding: None.},
author = {Tschandl, Philipp and Codella, Noel and Akay, Beng{\"{u}} Nisa and Argenziano, Giuseppe and Braun, Ralph P. and Cabo, Horacio and Gutman, David and Halpern, Allan and Helba, Brian and Hofmann-Wellenhof, Rainer and Lallas, Aimilios and Lapins, Jan and Longo, Caterina and Malvehy, Josep and Marchetti, Michael A. and Marghoob, Ashfaq and Menzies, Scott and Oakley, Amanda and Paoli, John and Puig, Susana and Rinner, Christoph and Rosendahl, Cliff and Scope, Alon and Sinz, Christoph and Soyer, H. Peter and Thomas, Luc and Zalaudek, Iris and Kittler, Harald},
doi = {10.1016/S1470-2045(19)30333-X},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tschandl et al. - 2019 - Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classi.pdf:pdf},
issn = {14745488},
journal = {The Lancet Oncology},
month = {jul},
number = {7},
pages = {938--947},
publisher = {Lancet Publishing Group},
title = {{Comparison of the accuracy of human readers versus machine-learning algorithms for pigmented skin lesion classification: an open, web-based, international, diagnostic study}},
volume = {20},
year = {2019}
}
@article{Sensoy2018,
abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.},
annote = {Segundo resultado no scopus. Tem github which is nice e parece simples {\'{a}} primeira vista},
archivePrefix = {arXiv},
arxivId = {1806.01768},
author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
eprint = {1806.01768},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sensoy, Kaplan, Kandemir - 2018 - Evidential Deep Learning to Quantify Classification Uncertainty.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {3179--3189},
publisher = {Neural information processing systems foundation},
title = {{Evidential Deep Learning to Quantify Classification Uncertainty}},
url = {http://arxiv.org/abs/1806.01768 https://muratsensoy.github.io/uncertainty.html},
volume = {2018-Decem},
year = {2018}
}
@misc{ieeta-review,
abstract = {ieeta-review},
author = {{F. Silva}, P. Georgieva},
file = {:home/fabio/Desktop/IEETA{\_}overview{\_}AutomatedSkinLesionDiagnosis{\_}01Nov2018.pdf:pdf},
keywords = {computer-assisted dermatology,deep learning,machine learning,medical imaging,mobile applications,self-surveillance,skin lesions},
pages = {1--25},
title = {{Computer-Assisted Diagnosis of Skin Lesions : An Overview from Self-Surveillance to Deep Learning}},
year = {2018}
}
@article{talbot,
abstract = {Model selection strategies for machine learning algorithms typically involve the numerical opti-misation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.},
author = {Cawley, Gavin C and Talbot, Nicola L C},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cawley, Talbot - 2010 - On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bias-variance trade-off,model selection,over-fitting,performance evaluation,selection bias},
pages = {2079--2107},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
volume = {11},
year = {2010}
}
@phdthesis{maia,
abstract = {Transfer learning is a popular solution to the common problem in deep learning that is the lack of data or the computational resources to train large models from scratch, which skin lesion classification is a prime candidate for because high quality medical imaging data in this domain is scarce. This dissertation studies transfer learning in the domain of skin lesion classification by exploring pre-trained models of the VGG16 architecture (originally trained on ImageNet) and repurposing them for skin lesion classification on the ISIC 2018 dataset. Specifically, models of VGG16 are tested by exhaustively testing the layers at which weights are extracted from and up to which they are frozen from further training, concluding that extracting all layers from VGG16 and fine-tuning the last two convolutional blocks to the ISIC 2018 dataset is the most performant configuration. However different choices of optimizer and learning rates could unveil better models. For comparison, two custom CNN architectures are explored and trained from scratch in a typical end- to-end learning scheme, from which it can be seen that end-to-end learning of CNN is much harder due to the many different hyperparameters that need to be cross-validated on a wide range of values which is computationally intensive to do thoroughly. In conclusion, transfer learning is a much more practical strategy for skin lesion classification and most other computer vision problems.},
author = {Maia, F{\'{a}}bio},
file = {:home/fabio/mestrado/dissertacao/related{\_}thesis/fabio{\_}maia/FabioMaia{\_}Dissertation{\_}TransferLearningSkinLesionClassification.pdf:pdf},
keywords = {binary classification,convolutional neural network,deep learning,medical imaging,skin lesion diagnosis,transfer learning},
school = {University of Aveiro},
title = {{A Study of Transfer Learning for Skin Lesion Classificatio}},
year = {2019}
}
@article{Brinker2018,
abstract = {BACKGROUND State-of-the-art classifiers based on convolutional neural networks (CNNs) were shown to classify images of skin cancer on par with dermatologists and could enable lifesaving and fast diagnoses, even outside the hospital via installation of apps on mobile devices. To our knowledge, at present there is no review of the current work in this research area. OBJECTIVE This study presents the first systematic review of the state-of-the-art research on classifying skin lesions with CNNs. We limit our review to skin lesion classifiers. In particular, methods that apply a CNN only for segmentation or for the classification of dermoscopic patterns are not considered here. Furthermore, this study discusses why the comparability of the presented procedures is very difficult and which challenges must be addressed in the future. METHODS We searched the Google Scholar, PubMed, Medline, ScienceDirect, and Web of Science databases for systematic reviews and original research articles published in English. Only papers that reported sufficient scientific proceedings are included in this review. RESULTS We found 13 papers that classified skin lesions using CNNs. In principle, classification methods can be differentiated according to three principles. Approaches that use a CNN already trained by means of another large dataset and then optimize its parameters to the classification of skin lesions are the most common ones used and they display the best performance with the currently available limited datasets. CONCLUSIONS CNNs display a high performance as state-of-the-art skin lesion classifiers. Unfortunately, it is difficult to compare different classification methods because some approaches use nonpublic datasets for training and/or testing, thereby making reproducibility difficult. Future publications should use publicly available benchmarks and fully disclose methods used for training to allow comparability.},
author = {Brinker, Titus Josef and Hekler, Achim and Utikal, Jochen Sven and Grabe, Niels and Schadendorf, Dirk and Klode, Joachim and Berking, Carola and Steeb, Theresa and Enk, Alexander H and von Kalle, Christof},
doi = {10.2196/11936},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brinker et al. - 2018 - Skin Cancer Classification Using Convolutional Neural Networks Systematic Review.pdf:pdf},
issn = {1438-8871},
journal = {Journal of medical Internet research},
keywords = {carcinoma classification,convolutional neural networks,deep learning,lesion classification,melanoma classification,skin cancer},
month = {oct},
number = {10},
pages = {e11936},
pmid = {30333097},
publisher = {Journal of Medical Internet Research},
title = {{Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30333097},
volume = {20},
year = {2018}
}
@article{efficientnet,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {1905.11946},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:pdf},
month = {may},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@inproceedings{resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2016 - Deep residual learning for image recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
publisher = {IEEE Computer Society},
title = {{Deep residual learning for image recognition}},
url = {https://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Baylor2017,
abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components-a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components , simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2{\%} increase in app installs resulting from improved data and model analysis.},
author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and {Yu Foo}, Chuan and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and {Yuen Koo}, Chiu and Lew, Lukasz and Mewald, Clemens and {Naresh Modi}, Akshay and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and {Euijong Whang}, Steven and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
doi = {10.1145/3097983.3098021},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baylor et al. - 2017 - TFX A TensorFlow-Based Production-Scale Machine Learning Platform.pdf:pdf},
title = {{TFX: A TensorFlow-Based Production-Scale Machine Learning Platform}},
url = {http://dx.doi.org/10.1145/3097983.3098021},
year = {2017}
}
@misc{isic2019first,
abstract = {In this paper we describe our method for the ISIC 2019 Skin Lesion Classification Challenge. The challenge's goal is to classify skin lesions based on dermoscopic images. A diverse dataset of 25 000 images was provided for training, containing images from eight classes. The final test set contains an additional, unknown class. We address this challenging problem with a simple, data driven approach by including external data with skin lesions types that are not present in the training set. Furthermore , multi-class skin lesion classification comes with the problem of severe class imbalance. We try to overcome this problem using loss balancing. Also, the dataset contains images with very different resolutions. We take care of this property by considering different model input resolutions and different cropping strategies. We aggregate all our models with an ensembling strategy where we search for the optimal subset of models. Our final ensemble achieves a balanced accuracy of 70 ± 5 {\%} using five-fold cross-validation.},
author = {Gessert, Nils and Nielsen, Maximilian and Shaikh, Mohsin and Werner, Ren{\'{e}} and Schlaefer, Alexander},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gessert et al. - Unknown - Skin Lesion Classification Using Loss Balancing and Ensembles of Multi-Resolution EfficientNets.pdf:pdf},
keywords = {Deep Learning,EfficientNet,Loss Balancing,Skin Lesion Classification},
title = {{Skin Lesion Classification Using Loss Balancing and Ensembles of Multi-Resolution EfficientNets}},
year = {2019}
}
@misc{Ng,
abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L 1 regu-larization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L 1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm-including logistic regression with L 2 regularization, SVMs, and neural networks trained by backpropagation-has a worst case sample complexity that grows at least linearly in the number of irrelevant features .},
author = {Ng, Andrew},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - Unknown - Feature selection, L 1 vs. L 2 regularization, and rotational invariance.pdf:pdf},
title = {{Feature selection, L 1 vs. L 2 regularization, and rotational invariance}}
}
@article{Engineeringa,
author = {Engineering, Audiovisual Systems},
file = {:home/fabio/mestrado/dissertacao/related{\_}thesis/BSc{\_}thesis.pdf:pdf},
number = {January 2017},
title = {{SKIN LESION DETECTION FROM DERMOSCOPIC IMAGES USING CONVOLUTIONAL NEURAL NETWORKS Submitted to the Faculty of the Escola T ` o de Barcelona a Romero L ´ opez In partial fulfillment of the requirements for the degree in Audiovisual Systems Engineering Advi}}
}
@article{Tschandl2019,
abstract = {Importance: Convolutional neural networks (CNNs) achieve expert-level accuracy in the diagnosis of pigmented melanocytic lesions. However, the most common types of skin cancer are nonpigmented and nonmelanocytic, and are more difficult to diagnose. Objective: To compare the accuracy of a CNN-based classifier with that of physicians with different levels of experience. Design, Setting, and Participants: A CNN-based classification model was trained on 7895 dermoscopic and 5829 close-up images of lesions excised at a primary skin cancer clinic between January 1, 2008, and July 13, 2017, for a combined evaluation of both imaging methods. The combined CNN (cCNN) was tested on a set of 2072 unknown cases and compared with results from 95 human raters who were medical personnel, including 62 board-certified dermatologists, with different experience in dermoscopy. Main Outcomes and Measures: The proportions of correct specific diagnoses and the accuracy to differentiate between benign and malignant lesions measured as an area under the receiver operating characteristic curve served as main outcome measures. Results: Among 95 human raters (51.6{\%} female; mean age, 43.4 years; 95{\%} CI, 41.0-45.7 years), the participants were divided into 3 groups (according to years of experience with dermoscopy): beginner raters ({\textless}3 years), intermediate raters (3-10 years), or expert raters ({\textgreater}10 years). The area under the receiver operating characteristic curve of the trained cCNN was higher than human ratings (0.742; 95{\%} CI, 0.729-0.755 vs 0.695; 95{\%} CI, 0.676-0.713; P {\textless}.001). The specificity was fixed at the mean level of human raters (51.3{\%}), and therefore the sensitivity of the cCNN (80.5{\%}; 95{\%} CI, 79.0{\%}-82.1{\%}) was higher than that of human raters (77.6{\%}; 95{\%} CI, 74.7{\%}-80.5{\%}). The cCNN achieved a higher percentage of correct specific diagnoses compared with human raters (37.6{\%}; 95{\%} CI, 36.6{\%}-38.4{\%} vs 33.5{\%}; 95{\%} CI, 31.5{\%}-35.6{\%}; P =.001) but not compared with experts (37.3{\%}; 95{\%} CI, 35.7{\%}-38.8{\%} vs 40.0{\%}; 95{\%} CI, 37.0{\%}-43.0{\%}; P =.18). Conclusions and Relevance: Neural networks are able to classify dermoscopic and close-up images of nonpigmented lesions as accurately as human experts in an experimental setting.},
author = {Tschandl, Philipp and Rosendahl, Cliff and Akay, Bengu Nisa and Argenziano, Giuseppe and Blum, Andreas and Braun, Ralph P. and Cabo, Horacio and Gourhant, Jean Yves and Kreusch, J{\"{u}}rgen and Lallas, Aimilios and Lapins, Jan and Marghoob, Ashfaq and Menzies, Scott and Neuber, Nina Maria and Paoli, John and Rabinovitz, Harold S. and Rinner, Christoph and Scope, Alon and Soyer, H. Peter and Sinz, Christoph and Thomas, Luc and Zalaudek, Iris and Kittler, Harald},
doi = {10.1001/jamadermatol.2018.4378},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tschandl et al. - 2019 - Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks.pdf:pdf;:home/fabio/Downloads/doi180064supp1{\_}prod.pdf:pdf},
issn = {21686068},
journal = {JAMA Dermatology},
keywords = {convolutional neural networks,dermoscopy,skin cancer,skin lesion},
month = {jan},
number = {1},
pages = {58--65},
publisher = {American Medical Association},
title = {{Expert-Level Diagnosis of Nonpigmented Skin Cancer by Combined Convolutional Neural Networks}},
volume = {155},
year = {2019}
}
@misc{Ly2019,
abstract = {Deep learning neural networks have made significant progress in image analysis and have been used for skin cancer recognition. Early detection and proper treatments for malignant skin cancer cases are vital to ensure high survival rate in patients. We present a novel deep learning based convolutional neural network (CNN) model for generating compatible models on mobile platforms such as Android and iOS. The proposed model was tested on the grand challenge PHDB melanoma dataset. The best performing proposed model excels in the following ways: (1) it outperforms the baseline model in terms of accuracy by 1{\%}; (2) it consists of 60{\%} fewer parameters compared to the base model and thereby it is more efficient on mobile platforms. Furthermore, the model is more compact and retains high accuracy without the need to be downsized; (3) in conjunction with advanced regularization techniques such as dropout and data augmentation, the proposed CNN model excelled when implemented on state-of-the-art frameworks such as Keras and TensorFlow. Additionally, we were able to successfully deploy it on the iOS and Android mobile systems. The proposed model could also be lucrative towards other datasets for image classification on mobile platform},
author = {Ly, Phillip and Bein, Doina and Verma, Abhishek},
doi = {10.1109/uemcon.2018.8796628},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ly, Bein, Verma - 2019 - New Compact Deep Learning Model for Skin Cancer Recognition(2).pdf:pdf},
isbn = {9781538676936},
keywords = {CNN,Deep Learning,Melanoma,Mobile Systems,PHDB,Skin Cancer},
month = {aug},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{New Compact Deep Learning Model for Skin Cancer Recognition}},
year = {2019}
}
@book{Nielsen2017a,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
file = {:home/fabio/mestrado/dissertacao/books/NeuralNetwroks {\&} DeepLearning ebook by Michael Nielsen.pdf:pdf},
pages = {389--411},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2018}
}
@misc{ieeta-collaboration,
author = {Silva, Filipe and Georgieva, P{\'{e}}tia},
file = {:home/fabio/Desktop/IEETA{\_}EPIDERMIS{\_}CollaborativeProposal{\_}01Nov2018.pdf:pdf},
title = {{Machine Learning for Automated Diagnosis of Pigmented Skin Lesions}},
year = {2018}
}
@inproceedings{inceptionv1,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
month = {oct},
pages = {1--9},
publisher = {IEEE Computer Society},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Hendrycks2019,
abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
annote = {Terceiro resultado no scopus (mas muito citado fora do scopus). Tem github mas parece complicado},
archivePrefix = {arXiv},
arxivId = {1610.02136},
author = {Hendrycks, Dan and Gimpel, Kevin},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1610.02136},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hendrycks, Gimpel - 2019 - A baseline for detecting misclassified and out-of-distribution examples in neural networks.pdf:pdf},
month = {oct},
publisher = {International Conference on Learning Representations, ICLR},
title = {{A baseline for detecting misclassified and out-of-distribution examples in neural networks}},
url = {https://github.com/hendrycks/error-detection},
year = {2019}
}
@misc{dermengine,
title = {{DermEngine | Visual Search}},
url = {https://www.dermengine.com/en-ca/visual-search},
urldate = {2019-11-21}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://github.com/fchollet/keras{\}}},
publisher = {GitHub},
title = {{Keras}},
year = {2015}
}
@article{gessert2018,
abstract = {In this paper we present the methods of our submission to the ISIC 2018 challenge for skin lesion diagnosis (Task 3). The dataset consists of 10000 images with seven image-level classes to be distinguished by an automated algorithm. We employ an ensemble of convolutional neural networks for this task. In particular, we fine-tune pretrained state-of-the-art deep learning models such as Densenet, SENet and ResNeXt. We identify heavy class imbalance as a key problem for this challenge and consider multiple balancing approaches such as loss weighting and balanced batch sampling. Another important feature of our pipeline is the use of a vast amount of unscaled crops for evaluation. Last, we consider meta learning approaches for the final predictions. Our team placed second at the challenge while being the best approach using only publicly available data.},
archivePrefix = {arXiv},
arxivId = {1808.01694},
author = {Gessert, Nils and Sentker, Thilo and Madesta, Frederic and Schmitz, R{\"{u}}diger and Kniep, Helge and Baltruschat, Ivo and Werner, Ren{\'{e}} and Schlaefer, Alexander},
eprint = {1808.01694},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gessert et al. - 2018 - Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation and Loss Weighting.pdf:pdf},
month = {aug},
title = {{Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation and Loss Weighting}},
url = {http://arxiv.org/abs/1808.01694},
year = {2018}
}
@techreport{mixup,
archivePrefix = {arXiv},
arxivId = {1710.09412v2},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
eprint = {1710.09412v2},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - mixup BEYOND EMPIRICAL RISK MINIMIZATION.pdf:pdf},
title = {{mixup: BEYOND EMPIRICAL RISK MINIMIZATION}},
url = {https://github.com/facebookresearch/mixup-cifar10.}
}
@article{Esteva2017,
abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Diagnosis,Machine learning,Skin cancer},
month = {feb},
number = {7639},
pages = {115--118},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://www.nature.com/articles/nature21056},
volume = {542},
year = {2017}
}
@article{Mar2018,
author = {Mar, V. J. and Soyer, H. P.},
doi = {10.1093/annonc/mdy193},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mar, Soyer - 2018 - Artificial intelligence for melanoma diagnosis How can we deliver on the promise.pdf:pdf},
issn = {15698041},
journal = {Annals of Oncology},
number = {8},
publisher = {Oxford University Press},
title = {{Artificial intelligence for melanoma diagnosis: How can we deliver on the promise?}},
volume = {29},
year = {2018}
}
@misc{marctransferlearning,
author = {Marcelino, Pedro},
title = {{Transfer learning from pre-trained models - Towards Data Science}},
url = {https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751},
urldate = {2019-12-02},
year = {2018}
}
@article{shadesgray,
author = {{Finlayson, Graham and Trezzi}, Elisabetta},
journal = {Proceedings of the 12th Color Imaging Conference},
pages = {37--41},
title = {{Shades of Gray and Colour Constancy}},
year = {2004}
}
@misc{yu,
abstract = {Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, respectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.},
author = {Yu, Lequan and Chen, Hao and Dou, Qi and Qin, Jing and Heng, Pheng Ann},
booktitle = {IEEE Transactions on Medical Imaging},
doi = {10.1109/TMI.2016.2642839},
file = {:home/fabio/Desktop/yu.pdf:pdf},
issn = {1558254X},
keywords = {Automated melanoma recognition,fully convolutional neural networks,residual learning,skin lesion analysis,very deep convolutional neural networks},
month = {apr},
number = {4},
pages = {994--1004},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks}},
volume = {36},
year = {2017}
}
@inproceedings{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
keywords = {adam},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@inproceedings{snapshot,
abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4{\%} and 17.4{\%} respectively.},
archivePrefix = {arXiv},
arxivId = {1704.00109},
author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1704.00109},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2019 - Snapshot ensembles Train 1, get M for free.pdf:pdf},
month = {apr},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Snapshot ensembles: Train 1, get M for free}},
year = {2019}
}
@misc{msk,
abstract = {This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classification. Participation involved 593 registrations, 81 pre-submissions, 46 finalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this field to date. While the official challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.},
archivePrefix = {arXiv},
arxivId = {1710.05006},
author = {Codella, Noel C. F.},
eprint = {1710.05006},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Codella et al. - 2017 - Skin Lesion Analysis Toward Melanoma Detection A Challenge at the 2017 International Symposium on Biomedical Ima.pdf:pdf},
month = {oct},
title = {{Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)}},
url = {http://arxiv.org/abs/1710.05006},
year = {2017}
}
@misc{isic2018,
title = {{Task 3: Lesion Diagnosis | ISIC 2018}},
url = {https://challenge2018.isic-archive.com/task3/},
urldate = {2019-11-23},
year = {2018}
}
@misc{ilsvrc,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
booktitle = {International Journal of Computer Vision},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{ham10000,
abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50{\%} of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
archivePrefix = {arXiv},
arxivId = {1803.10417},
author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
doi = {10.1038/sdata.2018.161},
eprint = {1803.10417},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tschandl, Rosendahl, Kittler - 2018 - Data descriptor The HAM10000 dataset, a large collection of multi-source dermatoscopic images of c.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
month = {aug},
publisher = {Nature Publishing Groups},
title = {{The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions}},
volume = {5},
year = {2018}
}
@misc{bcn_20000,
abstract = {This article summarizes the BCN20000 dataset, composed of 19424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Cl$\backslash$'inic in Barcelona. With this dataset, we aim to study the problem of unconstrained classification of dermoscopic images of skin cancer, including lesions found in hard-to-diagnose locations (nails and mucosa), large lesions which do not fit in the aperture of the dermoscopy device, and hypo-pigmented lesions. The BCN20000 will be provided to the participants of the ISIC Challenge 2019, where they will be asked to train algorithms to classify dermoscopic images of skin cancer automatically.},
archivePrefix = {arXiv},
arxivId = {1908.02288},
author = {Combalia, Marc and Codella, Noel C. F. and Rotemberg, Veronica and Helba, Brian and Vilaplana, Veronica and Reiter, Ofer and Carrera, Cristina and Barreiro, Alicia and Halpern, Allan C. and Puig, Susana and Malvehy, Josep},
eprint = {1908.02288},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Combalia et al. - 2019 - BCN20000 Dermoscopic Lesions in the Wild.pdf:pdf},
month = {aug},
title = {{BCN20000: Dermoscopic Lesions in the Wild}},
url = {http://arxiv.org/abs/1908.02288},
year = {2019}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptation of feature detectors.pdf:pdf},
month = {jul},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@misc{Pathan2018,
abstract = {Computerized image analysis methods for dermoscopy are primarily of great interest and benefit, as it provides significant information about the lesion, which can be of pertinence for the clinicians and a stand-alone warning implement. Computer-based diagnostic systems require dedicated image processing algorithms to provide mathematical descriptions of the suspected regions, such systems hold a great potential in oncology. In this paper, we have performed a review of the state of art techniques used in computer-aided diagnostic systems, by giving the domain aspects of melanoma followed by the prominent techniques used in each of the steps. The steps include dermoscopic image pre-processing, segmentation, extraction and selection of peculiar features, and relegation of skin lesions. The paper also presents cognizance to judge the consequentiality of every methodology utilized in the literature, in addition to the corresponding results obtained in this perspective. The inadequacies and the future research directions are accentuated.},
author = {Pathan, Sameena and Prabhu, K. Gopalakrishna and Siddalingaswamy, P. C.},
booktitle = {Biomedical Signal Processing and Control},
doi = {10.1016/j.bspc.2017.07.010},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathan, Prabhu, Siddalingaswamy - 2018 - Techniques and algorithms for computer aided diagnosis of pigmented skin lesions—A review.pdf:pdf},
issn = {17468108},
keywords = {Acquisition,Classification,Dermoscopy,Melanoma,Pigmented skin lesions (PSLs),Segmentation},
month = {jan},
pages = {237--262},
publisher = {Elsevier Ltd},
title = {{Techniques and algorithms for computer aided diagnosis of pigmented skin lesions—A review}},
volume = {39},
year = {2018}
}
@techreport{Wang,
abstract = {This manuscript presents the approach used for ISIC 2019 Challenge Task 1: classify dermoscopic images among nine different diagnostic categories without meta-data. This approach consists of an ensemble of convolutional neural networks for classifying 8 known categories and an out-of-distribution detector to determine whether an image belongs to the unknown category. An extra out-of-distribution dataset containing 134 dermoscopic images was used to train the parameters of the detector. Unfortunately, the detector performed poorly in distinguishing in-and out-distribution samples. Only about 16{\%} of out-distribution samples were classified correctly. Consequently, the balanced multi-class accuracy was dropped from 0.838 to 0.754 for 8-and 9-category classification respectively.},
author = {Wang, Hsin-Wei},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - Unknown - Skin Lesion Classification using Ensemble of Convolutional Neural Networks and Out-of-Distribution Detector.pdf:pdf},
title = {{Skin Lesion Classification using Ensemble of Convolutional Neural Networks and Out-of-Distribution Detector}},
year = {2019}
}
@misc{Brownlee,
author = {Brownlee, Jason},
title = {{Ensemble Learning Methods for Deep Learning Neural Networks}},
url = {https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/},
urldate = {2020-03-16},
year = {2018}
}
@misc{March,
abstract = {Confirming a diagnosis of cutaneous melanoma requires obtaining a skin biopsy specimen. However, obtaining numerous biopsy specimens - which often happens in patients with increased melanoma risk - is associated with significant cost and morbidity. While some melanomas are easily recognized by the naked eye, many can be difficult to distinguish from nevi, and therefore there is a need and opportunity to develop new technologies that can facilitate clinical examination and melanoma diagnosis. In part I of this 2-part continuing medical education article, we will review the practical applications of emerging technologies for noninvasive melanoma diagnosis, including mobile (smartphone) applications, multispectral imaging (ie, MoleMate and MelaFind), and electrical impedance spectroscopy (Nevisense).},
author = {March, Jordon and Hand, Matthew and Grossman, Douglas},
booktitle = {Journal of the American Academy of Dermatology},
doi = {10.1016/j.jaad.2015.02.1138},
file = {:home/fabio/Desktop/1-s2.0-S0190962215013791-main.pdf:pdf},
issn = {10976787},
keywords = {Mela Find,Mole Mate,Nevisense,melanoma,mobile app,spectroscopy,teledermatology},
month = {jun},
number = {6},
pages = {929--941},
publisher = {Mosby Inc.},
title = {{Practical application of new technologies for melanoma diagnosis: Part I. Noninvasive approaches}},
volume = {72},
year = {2015}
}
@misc{Bastien,
abstract = {Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1211.5590v1},
author = {Bastien, Fr{\'{e}}d{\'{e}}ric and {Pascal Lamblin}, Nouizorg and Bergstra, James and Goodfellow, Ian and Bergeron, Arnaud and Bouchard, Nicolas and Warde-Farley, David and Bengio, Yoshua},
eprint = {1211.5590v1},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastien et al. - Unknown - Theano new features and speed improvements.pdf:pdf},
title = {{Theano: new features and speed improvements}}
}
@misc{vggnet,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
title = {{Very Deep Convolutional Networks For Large-Scale Image Recogntinion}},
url = {https://arxiv.org/pdf/1409.1556.pdf},
year = {2015}
}
@misc{Ali2017,
abstract = {In this paper, we propose a method for classifying melanoma images into benign and malignant using Convolutional Neural Networks (CNNs). Having an automated method for melanoma detection will assist dermatologists in the early diagnosis of this type of skin cancer. A regular convolutional network employing a modest number of parameters is used to detect melanoma images. The architecture is used to classify the dataset of the ISBI 2016 challenge in melanoma classification. The dataset was not segmented or cropped prior to classification. The proposed method was then evaluated for accuracy, sensitivity and specificity. Comparisons with the winning entry in the competition demonstrate that one can achieve a performance level comparable to state-of-the-art using standard convolutional neural network architectures that employ a lower number of parameters.},
author = {Ali, Aya Abu and Al-Marzouqi, Hasan},
booktitle = {2017 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2017},
doi = {10.1109/ICECTA.2017.8252041},
isbn = {9781538608722},
month = {jun},
pages = {1--5},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Melanoma detection using regular convolutional neural networks}},
volume = {2018-Janua},
year = {2017}
}
@article{Perez2018,
abstract = {Deep learning models show remarkable results in automated skin lesion analysis. However, these models demand considerable amounts of data, while the availability of annotated skin lesion images is often limited. Data augmentation can expand the training dataset by transforming input images. In this work, we investigate the impact of 13 data augmentation scenarios for melanoma classification trained on three CNNs (Inception-v4, ResNet, and DenseNet). Scenarios include traditional color and geometric transforms, and more unusual augmentations such as elastic transforms, random erasing and a novel augmentation that mixes different lesions. We also explore the use of data augmentation at test-time and the impact of data augmentation on various dataset sizes. Our results confirm the importance of data augmentation in both training and testing and show that it can lead to more performance gains than obtaining new images. The best scenario results in an AUC of 0.882 for melanoma classification without using external data, outperforming the top-ranked submission (0.874) for the ISIC Challenge 2017, which was trained with additional data.},
archivePrefix = {arXiv},
arxivId = {1809.01442},
author = {Perez, F{\'{a}}bio and Vasconcelos, Cristina and Avila, Sandra and Valle, Eduardo},
doi = {10.1007/978-3-030-01201-4_33},
eprint = {1809.01442},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez et al. - 2018 - Data Augmentation for Skin Lesion Analysis.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data augmentation,Deep learning,Skin lesion analysis},
month = {sep},
pages = {303--311},
publisher = {Springer Verlag},
title = {{Data Augmentation for Skin Lesion Analysis}},
url = {http://arxiv.org/abs/1809.01442 http://dx.doi.org/10.1007/978-3-030-01201-4{\_}33},
volume = {11041 LNCS},
year = {2018}
}
@book{DipanjanSarkarRaghavBali2018,
author = {{Dipanjan Sarkar, Raghav Bali}, Tamoghna Ghosh},
edition = {First},
publisher = {Packt Publishing},
title = {{Hands-On Transfer Learning with Python}},
year = {2018}
}
@article{Han2018,
abstract = {We tested the use of a deep learning algorithm to classify the clinical images of 12 skin diseases—basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, actinic keratosis, seborrheic keratosis, malignant melanoma, melanocytic nevus, lentigo, pyogenic granuloma, hemangioma, dermatofibroma, and wart. The convolutional neural network (Microsoft ResNet-152 model; Microsoft Research Asia, Beijing, China) was fine-tuned with images from the training portion of the Asan dataset, MED-NODE dataset, and atlas site images (19,398 images in total). The trained model was validated with the testing portion of the Asan, Hallym and Edinburgh datasets. With the Asan dataset, the area under the curve for the diagnosis of basal cell carcinoma, squamous cell carcinoma, intraepithelial carcinoma, and melanoma was 0.96 ± 0.01, 0.83 ± 0.01, 0.82 ± 0.02, and 0.96 ± 0.00, respectively. With the Edinburgh dataset, the area under the curve for the corresponding diseases was 0.90 ± 0.01, 0.91 ± 0.01, 0.83 ± 0.01, and 0.88 ± 0.01, respectively. With the Hallym dataset, the sensitivity for basal cell carcinoma diagnosis was 87.1{\%} ± 6.0{\%}. The tested algorithm performance with 480 Asan and Edinburgh images was comparable to that of 16 dermatologists. To improve the performance of convolutional neural network, additional images with a broader range of ages and ethnicities should be collected.},
author = {Han, Seung Seog and Kim, Myoung Shin and Lim, Woohyung and Park, Gyeong Hun and Park, Ilwoo and Chang, Sung Eun},
doi = {10.1016/j.jid.2018.01.028},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2018 - Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm(2).pdf:pdf;:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2018 - Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm.pdf:pdf},
issn = {15231747},
journal = {Journal of Investigative Dermatology},
month = {jul},
number = {7},
pages = {1529--1538},
publisher = {Elsevier B.V.},
title = {{Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm}},
volume = {138},
year = {2018}
}
@misc{Foundation2019,
abstract = {Skin Cancer Facts {\&} Statistics - The Skin Cancer Foundation},
title = {{Skin Cancer Facts {\&} Statistics - The Skin Cancer Foundation}},
url = {https://skincancer.org/skin-cancer-information/skin-cancer-facts/},
urldate = {2019-11-10},
year = {2019}
}
@article{Miotto2017,
abstract = {Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.},
author = {Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and Dudley, Joel T.},
doi = {10.1093/bib/bbx044},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miotto et al. - 2017 - Deep learning for healthcare Review, opportunities and challenges.pdf:pdf},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical informatics,Deep learning,Electronic health records,Genomics,Health care,Translational bioinformatics},
month = {may},
number = {6},
pages = {1236--1246},
publisher = {Oxford University Press},
title = {{Deep learning for healthcare: Review, opportunities and challenges}},
volume = {19},
year = {2017}
}
@article{Vyas2018,
abstract = {As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin {\$}m{\$} between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al.[7] and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.},
annote = {Foi usado no top 3 do ISIC 2019. N{\~{a}}o {\'{e}} muito popular portanto n{\~{a}}o usar na tese!!!!},
archivePrefix = {arXiv},
arxivId = {1809.03576},
author = {Vyas, Apoorv and Jammalamadaka, Nataraj and Zhu, Xia and Das, Dipankar and Kaul, Bharat and Willke, Theodore L.},
eprint = {1809.03576},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vyas et al. - 2018 - Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Anomaly detection,Out-of-distribution},
month = {sep},
pages = {560--574},
publisher = {Springer Verlag},
title = {{Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers}},
url = {http://arxiv.org/abs/1809.03576},
volume = {11212 LNCS},
year = {2018}
}
@misc{Celebi2019,
abstract = {Dermoscopy is a non-invasive skin imaging technique that permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. While studies on the automated analysis of dermoscopy images date back to the late 1990s, because of various factors (lack of publicly available datasets, open-source software, computational power, etc.), the field progressed rather slowly in its first two decades. With the release of a large public dataset by the International Skin Imaging Collaboration in 2016, development of open-source software for convolutional neural networks, and the availability of inexpensive graphics processing units, dermoscopy image analysis has recently become a very active research field. In this paper, we present a brief overview of this exciting subfield of medical image analysis, primarily focusing on three aspects of it, namely, segmentation, feature extraction, and classification. We then provide future directions for researchers.},
author = {Celebi, M. Emre and Codella, Noel and Halpern, Allan},
booktitle = {IEEE Journal of Biomedical and Health Informatics},
doi = {10.1109/JBHI.2019.2895803},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Celebi, Codella, Halpern - 2019 - Dermoscopy Image Analysis Overview and Future Directions.pdf:pdf},
issn = {21682194},
keywords = {Skin cancer,computer-aided diagnosis,dermoscopy,dermoscopy image analysis,melanoma},
month = {mar},
number = {2},
pages = {474--478},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Dermoscopy Image Analysis: Overview and Future Directions}},
volume = {23},
year = {2019}
}
@inproceedings{inceptionv2,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch normalization Accelerating deep network training by reducing internal covariate shift.pdf:pdf},
isbn = {9781510810587},
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
volume = {1},
year = {2015}
}
@misc{Haenssle2018,
abstract = {Background: Deep learning convolutional neural networks (CNN) May facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking. Methods: Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge. Results: In level-I dermatologists achieved a mean (6standard deviation) sensitivity and specificity for lesion classification of 86.6{\%} (69.3{\%}) and 71.3{\%} (611.2{\%}), respectively. More clinical information (level-II) improved the sensitivity to 88.9{\%} (69.6{\%}, P ¼ 0.19) and specificity to 75.7{\%} (611.7{\%}, P {\textless} 0.05). The CNN ROC curve revealed a higher specificity of 82.5{\%} when compared with dermatologists in level-I (71.3{\%}, P {\textless} 0.01) and level-II (75.7{\%}, P {\textless} 0.01) at their sensitivities of 86.6{\%} and 88.9{\%}, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P {\textless} 0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge. Conclusions: For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they May benefit from assistance by a CNN's image classification.},
author = {Haenssle, H. A. and Fink, C. and Schneiderbauer, R. and Toberer, F. and Buhl, T. and Blum, A. and Kalloo, A. and {Ben Hadj Hassen}, A. and Thomas, L. and Enk, A. and Uhlmann, L.},
booktitle = {Annals of Oncology},
doi = {10.1093/annonc/mdy166},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haenssle et al. - 2018 - Man against Machine Diagnostic performance of a deep learning convolutional neural network for dermoscopic mela.pdf:pdf},
issn = {15698041},
keywords = {Automated melanoma detection,Computer algorithm,Deep learning convolutional neural network,Dermoscopy,Melanocytic nevi,Melanoma},
number = {8},
pages = {1836--1842},
publisher = {Oxford University Press},
title = {{Man against Machine: Diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists}},
volume = {29},
year = {2018}
}
@techreport{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition , computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467v2},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
eprint = {1603.04467v2},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}}
}
@article{Greenspan2016,
author = {Greenspan, Hayit and {Van Ginneken}, Bram and Summers, Ronald M.},
doi = {10.1109/TMI.2016.2553401},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greenspan, Van Ginneken, Summers - 2016 - Guest Editorial Deep Learning in Medical Imaging Overview and Future Promise of an Exciting Ne.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
month = {may},
number = {5},
pages = {1153--1159},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique}},
volume = {35},
year = {2016}
}
@article{noisystudent,
abstract = {We present a simple self-training method that achieves 88.4{\%} top-1 accuracy on ImageNet, which is 2.0{\%} better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0{\%} to 83.7{\%}, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
archivePrefix = {arXiv},
arxivId = {1911.04252},
author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
eprint = {1911.04252},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2019 - Self-training with Noisy Student improves ImageNet classification.pdf:pdf},
month = {nov},
title = {{Self-training with Noisy Student improves ImageNet classification}},
url = {http://arxiv.org/abs/1911.04252},
year = {2019}
}
@article{Miller2018,
abstract = {Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society—forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100{\%}, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials.},
author = {Miller, D. Douglas and Brown, Eric W.},
doi = {10.1016/J.AMJMED.2017.10.035},
file = {:home/fabio/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Brown - 2018 - Artificial Intelligence in Medical Practice The Question to the Answer.pdf:pdf},
issn = {0002-9343},
journal = {The American Journal of Medicine},
month = {feb},
number = {2},
pages = {129--133},
publisher = {Elsevier},
title = {{Artificial Intelligence in Medical Practice: The Question to the Answer?}},
url = {https://www.sciencedirect.com/science/article/pii/S0002934317311178?via{\%}3Dihub},
volume = {131},
year = {2018}
}
